{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os, shutil, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Version of the packages in this work.'''\n",
    "# # print(\"Pandas version: \",pd.__version__)\n",
    "# print(\"Pandas version: 0.25.1\")\n",
    "# # print(\"Numpy version: \",np.__version__)\n",
    "# print(\"Numpy version: 1.16.4\")\n",
    "# # print(\"Sys python version: \",sys.version)\n",
    "# print(\"Sys python version: 3.7.1 | package by conda-forge [MSC v.1900 64 bit (AMD64)]\")\n",
    "# print(\"IPython: 7.8.0\")\n",
    "# print(\"IPython genutils: 0.2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing OCoClus method\n",
      "Real data clustering.\n",
      "C:\\Users\\yurin\\Documents\\my_github\\ococlus\n",
      "OCoClus' folder was cleaned.\n",
      "\n",
      "Dataset: Cal500\n",
      "Iterator:  1\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  2\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  3\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  4\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  5\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  6\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  7\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  8\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  9\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  10\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Find global co-cluster is done.\n",
      "\n",
      "Find overlapped co-clusters method\n",
      "\n",
      "Non-overlapped and overlapped co-clusters.\n",
      "Number of co-clusters:  90\n",
      "Output file saved in: ./OutputAnalysis/ococlus/Cal500/OCoClusResult_Cal500.txt\n",
      "\n",
      "Dataset: Covid19\n",
      "Iterator:  1\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  2\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  3\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  4\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  5\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  6\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  7\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  8\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  9\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Iterator:  10\n",
      "Find dense cocluster method.\n",
      "Expand co-cluster method.\n",
      "\n",
      "Find global co-cluster is done.\n",
      "\n",
      "Find overlapped co-clusters method\n",
      "\n",
      "Non-overlapped and overlapped co-clusters.\n",
      "Number of co-clusters:  132\n",
      "Output file saved in: ./OutputAnalysis/ococlus/Covid19/OCoClusResult_Covid19.txt\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "#toy example\n",
    "# input_data_pd = pd.read_csv(\"./data/toy_example/input_test2.dat\", header=None, names=[\"transation\"])\n",
    "\n",
    "#synthetic\n",
    "# path = \"./data/synthetic/fimi/\"\n",
    "# syn_datasets = [path+\"synthetic-1_fimi.dat\",path+\"synthetic-2_fimi.dat\",path+\"synthetic-3_fimi.dat\"]\n",
    "\n",
    "experiment = 'Real' # Syn: run the synthetic data analysis; Real: run the real data analysis; \n",
    "                   # Toy: run the toy example.\n",
    "find_overlap = True # True: find overlapped co-clusters; False: Find no overlapped co-clusters\n",
    "VERBOSE = False # True: print results as a verbose mode; False: print the main results\n",
    "num_of_sim = 1 # number of simulations to perform\n",
    "k = -1 #max number of co-cluster that could be found. -1: default [driven by cost function]\n",
    "e_obj = 0 # maximum error tolerance for object. -1: default [accept the maximum error]\n",
    "e_att = 0 # maximum error tolerance for attribute. -1: default [accept the maximum error]\n",
    "\n",
    "\n",
    "print(\"Executing OCoClus method\")\n",
    "if experiment == 'Syn':\n",
    "    path = \"./data/synthetic/fimi/\"\n",
    "    syn_datasets = [path+\"synthetic-1_fimi.dat\",path+\"synthetic-2_fimi.dat\",path+\"synthetic-3_fimi.dat\"]\n",
    "    path_method = \"OutputAnalysis\\ococlus\"\n",
    "    check_path(path_method)\n",
    "\n",
    "    for ds in range(len(syn_datasets)):\n",
    "        ds_name = \"Syn-\"+str(ds+1)\n",
    "        print(\"\\nDataset: \"+ds_name)\n",
    "        res = os.mkdir(path_method+\"\\\\\"+ds_name)\n",
    "\n",
    "        for run in range(num_of_sim):\n",
    "            print(\"Run-\"+str(run+1))\n",
    "\n",
    "            df_fimi = pd.read_csv(syn_datasets[ds], header=None, names=[\"transation\"])\n",
    "            D,co_clusters = OCoClus(df_fimi,k,e_obj,e_att) # Calling OCoClus main method\n",
    "\n",
    "            print(\"\")\n",
    "            Rec_error(D,co_clusters)\n",
    "#             print(co_clusters)\n",
    "            \n",
    "            omega_format = build_clustering_output_omega(co_clusters)\n",
    "            OCoClus_clustering_xm = xmeasures_format(omega_format)# save to XMEASURES format C++ version\n",
    "            df_gt = pd.DataFrame(OCoClus_clustering_xm)\n",
    "            path = path_method+\"/\"+ds_name\n",
    "            df_gt.to_csv(path.replace(\"\\\\\",\"/\")+\"/run_\"+str(run+1)+\"_res_ococlus_\"+ds_name+\"_co.cnl\", \n",
    "                         header= False,index=False, encoding='utf8')\n",
    "            del omega_format, df_gt, OCoClus_clustering_xm\n",
    "            gc.collect()\n",
    "elif experiment == 'Real':\n",
    "    k = 10\n",
    "    print('Real data clustering.')\n",
    "    path = \"./data/real_application/\"\n",
    "    real_datasets = [path+\"cal500_fimi.dat\",path+\"covid19_fimi.dat\"]\n",
    "    path_method = \"OutputAnalysis\\ococlus\"\n",
    "    check_path(path_method)\n",
    "    \n",
    "    for ds in range(len(real_datasets)):\n",
    "        ds_name = real_datasets[ds].replace(\"/\",\"_\").split(\"_\")[4].capitalize()\n",
    "        print(\"\\nDataset: \"+ds_name)\n",
    "        res = os.mkdir(path_method+\"\\\\\"+ds_name)\n",
    "        \n",
    "        df_fimi = pd.read_csv(real_datasets[ds], header=None, names=[\"transation\"])\n",
    "        D,co_clusters = OCoClus(df_fimi,k,e_obj,e_att) # Calling OCoClus main method\n",
    "        writeFileOutput(co_clusters,ds_name,method='OCoClus',fileName='OCoClusResult_'+ds_name)\n",
    "        \n",
    "    print('DONE!')\n",
    "elif experiment == 'Toy':\n",
    "    print('Toy example')\n",
    "    input_data_pd = pd.read_csv(\"./data/toy_example/input_test2.dat\", header=None, names=[\"transation\"])\n",
    "    D,co_clusters = OCoClus(input_data_pd,k,e_obj,e_att)\n",
    "    #Compute the measures\n",
    "#     print(\"\")\n",
    "    Rec_error(D,co_clusters)\n",
    "else:\n",
    "    print('ERROR! Choose a valid option for the experiment analysis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Omega index, overlapped F1, ONMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "File OutputAnalysis/ococlus/Syn-1/run_1_res_ococlus_Syn-1_co.cnl is not empty. It has 7 lines.\n",
      " \n",
      " \n",
      "= Overlaps Evaluation =\n",
      "MF1p_u (PARTPROB, UNWEIGHTED):\n",
      "1 (Prc: 1, Rec: 1)\n",
      "OI:\n",
      "1\n",
      "MF1p_u: 1 (Prc: 1, Rec: 1); OI: 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "### omega index and f-score\n",
    "### ./xmeasures -o -fp -ku -O gt.txt cls2.txt\n",
    "# pwd\n",
    "# ls\n",
    "\n",
    "if [ -d \"xmeasures/OutputAnalysis/ococlus\" ] \n",
    "then\n",
    "#     echo \"Directory exists.\"\n",
    "    rm -R xmeasures/OutputAnalysis/ococlus\n",
    "else\n",
    "    echo \"Error: Directory does not exists.\"\n",
    "fi\n",
    "\n",
    "cp -R OutputAnalysis xmeasures\n",
    "cd xmeasures/\n",
    "\n",
    "#Method: [Ococlus]\n",
    "#ground-truth: [gt_xm_s1_co.cnl,gt_xm_s2_co.cnl,gt_xm_s3_co.cnl]\n",
    "for i in 1 2 3 # index of synthetic datasets\n",
    "# for i in 1\n",
    "do\n",
    "#     for file in OutputAnalysis/ococlus/Syn-${i}/*\n",
    "    for file in OutputAnalysis/ococlus/Syn-${i}/*\n",
    "    do\n",
    "    #     echo \" $(grep -c '' ${file})\"\n",
    "        res=$(grep -c '' ${file})\n",
    "    #     echo \"${file} lines equal to: ${res}\"\n",
    "\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "    #         echo \"Empty file\"\n",
    "            ./xmeasures -o -fp -ku -O ./gts/gt_xm_s${i}_trad.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "#     # wait until all child processes are done\n",
    "#     wait\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "File OutputAnalysis/ococlus/Syn-1/run_1_res_ococlus_Syn-1_co.cnl is not empty. It has 7 lines.\n",
      " \n",
      " \n",
      "# Average estimated membership in './gts/gt_xm_s1_trad.cnl': 0.6644 (3790 / 5704)\n",
      "# Average estimated membership in 'OutputAnalysis/ococlus/Syn-1/run_1_res_ococlus_Syn-1_co.cnl': 0.6644 (3790 / 5704)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "#### ONMI\n",
    "#### ./onmi file1 file2\n",
    "\n",
    "cd xmeasures\n",
    "\n",
    "#Method: [Ococlus]\n",
    "#ground-truth: [gt_xm_s1_trad.cnl,gt_xm_s2_trad.cnl,gt_xm_s3_trad.cnl]\n",
    "for i in 1 2 3 # index of synthetic datasets\n",
    "# for i in 1\n",
    "do\n",
    "    for file in OutputAnalysis/ococlus/Syn-${i}/*\n",
    "    do\n",
    "    #     echo \" $(grep -c '' ${file})\"\n",
    "        res=$(grep -c '' ${file})\n",
    "    #     echo \"${file} lines equal to: ${res}\"\n",
    "\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "    #         echo \"Empty file\"\n",
    "            ./onmi ./gts/gt_xm_s${i}_trad.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "#     # wait until all child processes are done\n",
    "#     wait\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCoClus algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main algorithm of OCoClus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def OCoClus(input_D,k=-1,e_obj=-1,e_att=-1):\n",
    "    ### variable declaration\n",
    "    if k == -1:\n",
    "        k=sys.maxsize\n",
    "    if e_obj == -1:\n",
    "        e_obj = 1\n",
    "    if e_att == -1:\n",
    "        e_att = 1\n",
    "    \n",
    "    cost_model = sys.float_info.max # initial cost function of the model\n",
    "    num_of_coclusters = 0\n",
    "    final_coclusters = [] # store the attribute and objects clusters. final_coclusters[[C1_att,C1_obj],[Ck_att,Ck_obj]]\n",
    "    pattern_model = [set(),set()]# Union between the found co-clusters [list of obj,list of att]\n",
    "    cost_per_cocluster = []# stores the cost to build the cocluster\n",
    "    history_cost_model = []\n",
    "    ###\n",
    "    \n",
    "    D,N,data_dict,data_res_dict = get_data(input_D)\n",
    "    for itertator in range(1,k+1):\n",
    "        print(\"Iterator: \",itertator)\n",
    "        if VERBOSE:\n",
    "            print(\"Pattern model: \",pattern_model)\n",
    "        C,E,new_cost_dense = find_dense_cocluster(data_dict, data_res_dict)\n",
    "        if new_cost_dense >= 0:\n",
    "            print(\"No relevant co-cluster can be found anymore.\")\n",
    "            break    \n",
    "        C_expanded = expand_dense_cocluster(C, E, data_dict,e_obj,e_att)\n",
    "\n",
    "        print(\"\")\n",
    "        new_cost = cost_function(len(pattern_model[0].union(C[1])),\n",
    "                                              len(pattern_model[1].union(set(C[0]))))\n",
    "        if VERBOSE:\n",
    "            print(\"Dense co-cluster cost: \",new_cost_dense)\n",
    "            print(\"Current model cost: \",cost_model)\n",
    "            print(\"New model cost with the found co-cluster: \",new_cost)\n",
    "            print(\"Attribute cluster:\"+str(C[0])+\", Object cluster: \"+str(C[1]))\n",
    "            print(\" \")\n",
    "        \n",
    "        if new_cost < cost_model:\n",
    "            final_coclusters.append(C)\n",
    "            cost_model = new_cost\n",
    "            num_of_coclusters += 1\n",
    "            data_res_dict = update_residual_dataset(data_res_dict,C[0],C[1])\n",
    "            pattern_model[0] = pattern_model[0].union(C[1])\n",
    "            pattern_model[1] = pattern_model[1].union(set(C[0]))\n",
    "        else:\n",
    "            print(\"No co-cluster can be found anymore.\")\n",
    "            break\n",
    "    \n",
    "    print(\"Find global co-cluster is done.\")\n",
    "    if find_overlap:\n",
    "        attributes_per_cluster = []\n",
    "        objects_per_cluster = []\n",
    "        for i in range(len(final_coclusters)):\n",
    "            attributes_per_cluster.append(set(list(map(int,final_coclusters[i][0]))))\n",
    "            objects_per_cluster.append(final_coclusters[i][1])\n",
    "\n",
    "        print(\"\\nFind overlapped co-clusters method\")\n",
    "        attribute_clusters, objects_clusters = findOverlap(attributes_per_cluster,objects_per_cluster)\n",
    "\n",
    "        final_coclusters = []\n",
    "        for i in range(len(attribute_clusters)):\n",
    "            final_coclusters.append([attribute_clusters[i],objects_clusters[i]])\n",
    "\n",
    "        print(\"\\nNon-overlapped and overlapped co-clusters.\")\n",
    "        print(\"Number of co-clusters: \",len(final_coclusters))\n",
    "        if VERBOSE:\n",
    "            print(\"Final co-clusters: \",final_coclusters)\n",
    "    else:\n",
    "        print(\"\\nNon-overlapped co-clusters.\")\n",
    "        print(\"Number of co-clusters: \",num_of_coclusters)\n",
    "#         for i in range(len(final_coclusters)):\n",
    "#             tmp_data = final_coclusters[i][1]\n",
    "#             final_coclusters[i][1] = list(tmp_data)\n",
    "        if VERBOSE:\n",
    "            print(\"Final co-clusters: \",final_coclusters)\n",
    "\n",
    "    return D,final_coclusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find dense co-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dense_cocluster(input_dataset, residual_dataset):\n",
    "    '''\n",
    "    Input\n",
    "        input_dataset: a dictionary with the list of objects per attribute. input_dataset[att] -> [objects_in_att]\n",
    "        residual_dataset: it is a copy of input_dataset used to sort the attributes\n",
    "    \n",
    "    Output\n",
    "        C: A dense co-cluster. It is a array with a list of attributes and a set of objects. C[att_list,obj_set]\n",
    "    '''\n",
    "    print(\"Find dense cocluster method.\")\n",
    "    att_dense_cocluster_list = []\n",
    "#     att_dense_cocluster_set = set([])\n",
    "    obj_dense_cocluster_list = []\n",
    "    att_extension_list = []\n",
    "    cost_dense_cocluster = sys.float_info.max\n",
    "    \n",
    "    sads = sort_att_ds(residual_dataset)\n",
    "#     print(\"Sorted att: \",sads)\n",
    "    cc_att = sads[0]\n",
    "#     att_dense_cocluster_list.append(cc_att)\n",
    "    att_dense_cocluster_list.append(cc_att)\n",
    "    obj_dense_cocluter_set = set(residual_dataset[cc_att]) # get the objs forthe given attribute cc_att\n",
    "    count_group_att = 1\n",
    "#     cost_dense_cocluster = cost_function(len(pattern_model[0].union(obj_dense_cocluter_set)),\n",
    "#                                          len(pattern_model[1].union(set(cc_att))))\n",
    "    new_cost_function = cost_function(len(obj_dense_cocluter_set), count_group_att)\n",
    "    if VERBOSE:\n",
    "        print(\"New cost: \"+str(new_cost_function)+\", Cost dense: \"+str(cost_dense_cocluster))\n",
    "    cost_dense_cocluster = new_cost_function\n",
    "    \n",
    "    for next_att in range(1,len(sads)):\n",
    "        cc_att_test = sads[next_att]\n",
    "#         print(\"Attribute: \",cc_att_test)\n",
    "        curr_cc_att = set(residual_dataset[cc_att_test])\n",
    "        intersection_objs = obj_dense_cocluter_set.intersection(curr_cc_att)\n",
    "#         print(\"Intersection test: \",intersection_objs)\n",
    "        tmp = att_dense_cocluster_list.copy()\n",
    "        tmp.append(cc_att_test)\n",
    "#         new_cost_function = cost_function(len(pattern_model[0].union(intersection_objs)),\n",
    "#                                           len(pattern_model[1].union(set(tmp))))\n",
    "        new_cost_function = cost_function(len(intersection_objs),count_group_att+1)\n",
    "#         print(intersection_objs,curr_cc_att)\n",
    "#         print(len(intersection_objs),count_group_att+1)\n",
    "        if VERBOSE:\n",
    "            print(\"New cost: \"+str(new_cost_function)+\", Cost dense: \"+str(cost_dense_cocluster))\n",
    "            \n",
    "        if  new_cost_function <= cost_dense_cocluster:\n",
    "            att_dense_cocluster_list.append(cc_att_test)\n",
    "            obj_dense_cocluter_set = intersection_objs\n",
    "            cost_dense_cocluster = new_cost_function\n",
    "            count_group_att += 1\n",
    "        else:\n",
    "            att_extension_list.append(cc_att_test)\n",
    "    \n",
    "    C = [att_dense_cocluster_list, obj_dense_cocluter_set]\n",
    "    \n",
    "    # no good rectangle was found\n",
    "    if cost_dense_cocluster >= 0:\n",
    "        att_extension_list = []\n",
    "        C = [[],set()]\n",
    "    else:\n",
    "        C = [att_dense_cocluster_list, obj_dense_cocluter_set]\n",
    "    \n",
    "    return C, att_extension_list, cost_dense_cocluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand dense co-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dense_cocluster(C,E,att_data_dict,e_obj,e_att):\n",
    "    '''\n",
    "    INPUT\n",
    "        C = a tuple(list_atts, set_objs); list_atts has the list of attributes and set_objs is a set of objects\n",
    "        E = list of attributes not present in C_a to be tested\n",
    "        att_data_dict = a dict with list of objects per att; att_data_dict[att] -> [objects]\n",
    "        e_obj = maximum object error \n",
    "        e_att = maximum attribute error \n",
    "    \n",
    "    OUTPUT\n",
    "        \n",
    "    '''\n",
    "    print(\"Expand co-cluster method.\")\n",
    "    added_att = True\n",
    "    \n",
    "    if not C[0]: # nothing good to discover\n",
    "        pass\n",
    "    else:\n",
    "        curr_cost = cost_function(len(C[1]), len(C[0]),0,0)\n",
    "        noise_added = 0 # the quantanty of noise added in the cluster during the process\n",
    "        while(added_att):\n",
    "            # try to add new objects to cocluster C\n",
    "            try_new_objs = set(np.arange(0,len(D))).difference(C[1]) # get the objects not present in C\n",
    "            if VERBOSE:\n",
    "                print(\"# Try to extend the list of Objects #\")\n",
    "            for obj in try_new_objs:\n",
    "                obj_quantanty = 0\n",
    "                for att in C[0]:\n",
    "                    if D[obj][int(att)] == 1:\n",
    "                        obj_quantanty += 1\n",
    "                if not_too_noisy(obj_quantanty, C, e_obj, e_att, att_data_dict, E, \"obj\"):\n",
    "#                     print(\"Ru√≠do valor: \",not_noise_val)\n",
    "                    new_cost = cost_function(len(C[1])+1,len(C[0]),0,(noise_added+(len(C[0])-obj_quantanty)))\n",
    "                    if new_cost <= curr_cost:\n",
    "                        C[1].add(obj)\n",
    "                        curr_cost = new_cost\n",
    "                        noise_added += (len(C[0])-obj_quantanty)\n",
    "                else:\n",
    "                    if VERBOSE:\n",
    "                        print(\"Object too noisy to be added. (Obj: \"+str(obj)+\")\")\n",
    "\n",
    "            added_att = False\n",
    "            # try to add new attributes to cocluster C\n",
    "            if VERBOSE:\n",
    "                print(\"# Try to extend the list of Attributes #\")\n",
    "            \n",
    "            while(len(E) != 0):\n",
    "                att = E.pop(0)\n",
    "                if VERBOSE:\n",
    "                    print(\"ATT->\"+str(att), end= \" | \")\n",
    "                att_obj_quantanty = len(C[1].intersection(set(att_data_dict[att])))\n",
    "                if not_too_noisy(att_obj_quantanty, C, e_obj, e_att, att_data_dict, E, \"att\"):\n",
    "                    new_cost = cost_function(len(C[1]),len(C[0])+1,0,(noise_added+(len(C[1])-att_obj_quantanty)))\n",
    "                    if new_cost <= curr_cost:\n",
    "                        C[0].append(str(att))\n",
    "                        curr_cost = new_cost\n",
    "                        added_att = True\n",
    "                        noise_added += (len(C[1])-att_obj_quantanty)\n",
    "                        break\n",
    "                else:\n",
    "                    if VERBOSE:\n",
    "                        print(\"Attribute too noisy to be added. (Att: \"+str(att)+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find overlapped co-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findOverlap(SetsC,SetsR):\n",
    "    '''\n",
    "    INPUT\n",
    "        SetsC: It has K sets of attributes regarding each attribute cluster\n",
    "        SetsR: It has K sets of objects regarding each object cluster\n",
    "    \n",
    "    OUTPUT\n",
    "        columnClusters: A list with the attribute clusters\n",
    "        rowClusters: A list with the object clusters\n",
    "    '''\n",
    "    newSetsColumns = []\n",
    "    newSetsRows = []\n",
    "    \n",
    "    # merge sets that can overlap\n",
    "    merge(SetsC,SetsR,newSetsColumns,newSetsRows)\n",
    "\n",
    "    #Removing sets with redundant information\n",
    "    removeSubsets(newSetsColumns,newSetsRows)\n",
    "    \n",
    "    columnClusters = newSetsColumns\n",
    "    rowClusters = newSetsRows\n",
    "    return columnClusters,rowClusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support functions for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(input_data):\n",
    "    '''\n",
    "    INPUT\n",
    "        input_data: A panda dataframe of the input data file.\n",
    "    \n",
    "    OUTPUT\n",
    "        Create global variables from the input data file to be used for the methods.\n",
    "        D: A binary matrix from the input data.\n",
    "        N: A noise binary matrix with the same size of D.\n",
    "        data_dict: A dictionary to store D as a vertical representation.\n",
    "        data_res_dict: A copy of data_dict used to sort the attributes of D and find unconvered elements.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    data_pd = input_data\n",
    "    global data_dict\n",
    "    data_dict = {}\n",
    "    global data_res_dict\n",
    "    data_res_dict = {}\n",
    "    global D # input data as a binary matrix\n",
    "    global N # noise matrix with the same size of D\n",
    "    num_of_objects = 0\n",
    "    num_of_attributes = 0\n",
    "    \n",
    "    max_val_att = 0\n",
    "    for index, row in data_pd.iterrows():\n",
    "        num_of_objects+=1\n",
    "        object_data = row[0].split(\" \")\n",
    "\n",
    "        for attribute in object_data:\n",
    "            if attribute != \"\":\n",
    "                if int(attribute) > max_val_att:\n",
    "                    max_val_att = int(attribute)\n",
    "                if attribute in data_dict:\n",
    "                    data_dict[attribute].append(index)\n",
    "                else:\n",
    "                    data_dict[attribute] = [index]\n",
    "    data_res_dict = data_dict.copy()\n",
    "#     num_of_attributes = len(data_dict)\n",
    "    num_of_attributes = max_val_att+1\n",
    "    \n",
    "    D = np.zeros((num_of_objects,num_of_attributes),dtype=int)\n",
    "    for key, values in data_dict.items():\n",
    "        for line in values:\n",
    "            D[line][int(key)] = 1        \n",
    "    N = np.zeros((num_of_objects,num_of_attributes),dtype=int)\n",
    "    return D,N,data_dict,data_res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too noisy (line,col)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_too_noisy(count_presence, C, e_obj, e_att, att_data_dict, E, dimension):\n",
    "    num_of_atts = len(C[0])\n",
    "    num_of_objs = len(C[1])\n",
    "    if dimension == \"obj\":\n",
    "        # obj must be present in at least (1-e_obj).||C_a||\n",
    "        return count_presence >= ((1-e_obj) * num_of_atts) # return true if the obj is not too noisy\n",
    "    else:\n",
    "        # col must be present in at least (1-e_tt).||C_o||\n",
    "        return count_presence >= ((1-e_att) * num_of_objs) # return true if the att is not too noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(numOfObj, numOfAtt, cov=0,noise=0):\n",
    "    if VERBOSE:\n",
    "        print(\"Num. objs: {0:2d}, Num. att: {1:2d}, Num. noise: {2:2d}\".format(numOfObj,numOfAtt,noise))\n",
    "    return ((numOfObj+numOfAtt) - (numOfObj*numOfAtt)) + cov + (2*noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(SetsC, SetsR, newSetsColumns, newSetsRows):\n",
    "    num_of_cluster = len(SetsC)\n",
    "    \n",
    "    # keep the original cluster\n",
    "    for set_column_i in range(len(SetsC)):\n",
    "        newSetsRows.append(SetsR[set_column_i])\n",
    "        newSetsColumns.append(SetsC[set_column_i])\n",
    "    \n",
    "    revisit = True\n",
    "    while(revisit):\n",
    "        revisit = False\n",
    "        tmp_r = []\n",
    "        tmp_c = []\n",
    "        size = len(newSetsColumns)\n",
    "        for i in range(size-1):\n",
    "            first_r = newSetsRows[i]\n",
    "            first_c = newSetsColumns[i]\n",
    "            for j in range(i+1,size):\n",
    "                p_intersec_r = first_r.intersection(newSetsRows[j])\n",
    "                p_intersec_c = first_c.union(newSetsColumns[j])\n",
    "                if len(p_intersec_r) > 0:\n",
    "                    tmp_r.append(p_intersec_r)\n",
    "                    tmp_c.append(p_intersec_c)\n",
    "        \n",
    "        for i in range(len(tmp_c)):\n",
    "            change = True\n",
    "            for j in range(len(newSetsColumns)):\n",
    "                if tmp_c[i].issubset(newSetsColumns[j]):\n",
    "                    change = False\n",
    "            if change:\n",
    "                newSetsRows.append(tmp_r[i])\n",
    "                newSetsColumns.append(tmp_c[i])\n",
    "                revisit = True\n",
    "    \n",
    "    # start to find and merge all possible overlapping clusters\n",
    "    for set_column_i in range(num_of_cluster):\n",
    "        tested_pattern = SetsR[set_column_i]\n",
    "        new_cols = SetsC[set_column_i]\n",
    "        overlapped = False\n",
    "        \n",
    "        # finds with who the tested pattern overlaps\n",
    "        set_overlap_id = []\n",
    "        for i in range(num_of_cluster):\n",
    "            if i != set_column_i:\n",
    "                if len(tested_pattern.intersection(SetsR[i])) > 0:\n",
    "                    set_overlap_id.append(i)    \n",
    "        \n",
    "        # Discover if exist overlaps between the sets in set_overlap_id\n",
    "        sub_overlap_id = []\n",
    "        for i in range(len(set_overlap_id)):\n",
    "            try_combine_ids = [set_overlap_id[i]]\n",
    "            added = False\n",
    "            for j in range(i+1,len(set_overlap_id)):\n",
    "                if len(SetsR[set_overlap_id[i]].intersection(SetsR[set_overlap_id[j]])) > 0:\n",
    "                    try_combine_ids.append(set_overlap_id[j])\n",
    "                    added = True\n",
    "            sub_overlap_id.append(try_combine_ids)\n",
    "        #Check if some pattern is isolated and was not added\n",
    "        for id_pattern in set_overlap_id:\n",
    "            added = False\n",
    "            for combined_ids in sub_overlap_id:\n",
    "                if id_pattern in combined_ids:\n",
    "                    added = True\n",
    "            if added == False:\n",
    "                sub_overlap_id.append([id_pattern])\n",
    "        \n",
    "        # merge the analysed pattern with the overlapped combined patterns ids\n",
    "        for pattern_ids in sub_overlap_id:\n",
    "            tested_pattern = SetsR[set_column_i]\n",
    "            new_cols = SetsC[set_column_i]\n",
    "            for pattern_id in pattern_ids:\n",
    "                tmp = tested_pattern.intersection(SetsR[pattern_id])\n",
    "                tested_pattern = tmp\n",
    "                new_cols = new_cols.union(SetsC[pattern_id])\n",
    "            # save the new patterns\n",
    "            newSetsColumns.append(new_cols)\n",
    "            newSetsRows.append(tested_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSubsets(setsColumns,setsRows):\n",
    "    finalSetsCols = []\n",
    "    finalSetsRows = []\n",
    "    \n",
    "    again = True\n",
    "    # We are out of the loop when we do not have any subset to remove\n",
    "    while(again):\n",
    "        again = False\n",
    "        for i in range(len(setsColumns)):\n",
    "            isSubset = False\n",
    "            currC = setsColumns[i]\n",
    "            currR = setsRows[i]\n",
    "            \n",
    "            for j in range(len(setsColumns)):\n",
    "                if i != j:\n",
    "                    nextC = setsColumns[j]\n",
    "                    nextR = setsRows[j]\n",
    "                    if currC.issubset(nextC) and currR.issubset(nextR):\n",
    "                        isSubset = True\n",
    "        \n",
    "            if isSubset or len(setsRows[i]) == 0:\n",
    "                setsColumns.pop(i)\n",
    "                setsRows.pop(i)\n",
    "                again = True\n",
    "                break\n",
    "    \n",
    "    #converting data type back to list\n",
    "    for i in range(len(setsColumns)):\n",
    "        setsColumns[i] = list(setsColumns[i])\n",
    "        setsRows[i] = list(setsRows[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort attributes in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_att_ds(data_res):\n",
    "    freq_res_dict = {}\n",
    "    for key,value in data_res.items():\n",
    "        freq_res_dict[key] = len(value)\n",
    "        \n",
    "    # Create a list of tuples sorted by index 1 i.e. value field     \n",
    "    listofTuples = sorted(freq_res_dict.items() , reverse=True, key=lambda x: x[1])\n",
    "    # Iterate over the sorted sequence\n",
    "    # for elem in listofTuples :\n",
    "    #     print(elem[0] , \" ::\" , elem[1] )\n",
    "#     print(listofTuples)\n",
    "    sorted_attributes = [elem[0] for elem in listofTuples]\n",
    "#     print(\"Sorted att: \",sorted_attributes)\n",
    "    return sorted_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update residual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_residual_dataset(res_data, attributes_cocluster, objects_cocluster):\n",
    "    for key, value in res_data.items():\n",
    "        if key in attributes_cocluster:\n",
    "            diff_objs = set(res_data[key]).difference(set(objects_cocluster))\n",
    "            res_data[key] = list(diff_objs)\n",
    "    return res_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results - check path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_path(path_method):\n",
    "    current_dir = os.getcwd()\n",
    "    print(current_dir)\n",
    "    res = os.path.exists(path_method)\n",
    "    # clean the folder to save new data\n",
    "    if res:\n",
    "        #check if it is empty\n",
    "        dir_empty = os.listdir(path_method)\n",
    "        if len(dir_empty) != 0:\n",
    "    #         shutil.rmtree(\"OutputAnalysis/kmeans/\")\n",
    "            rm = !rm -r --preserve-root './OutputAnalysis/ococlus/'*\n",
    "            if not rm:\n",
    "                print(\"OCoClus' folder was cleaned.\")\n",
    "    #             os.chdir(path_method)\n",
    "            else:\n",
    "                print(\"sad\")\n",
    "                print(rm)\n",
    "        else:\n",
    "    #         print(\"Empty!\")\n",
    "            pass\n",
    "    #         os.chdir(path_method)\n",
    "    else: # nothing exist so create it\n",
    "        # trying to insert to flase directory \n",
    "        try: \n",
    "    #         os.chdir(fd) \n",
    "            os.mkdir(path_method)\n",
    "            print(\"The path was created: \"+path_method)\n",
    "\n",
    "        # Caching the exception     \n",
    "        except: \n",
    "            print(\"Something wrong with specified directory. Exception- \", sys.exc_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save clustering result into a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def writeFileOutput(cols, rows, dataset, method='OCoClus', fileName='OCoClusResult'):\n",
    "def writeFileOutput(co_clusters, dataset, method='OCoClus', fileName='OCoClusResult'):\n",
    "    text = \"\"\n",
    "#    for c in range(len(data.rows_)):\n",
    "#        res = [i for i, val in enumerate(data.columns_[c]) if val]\n",
    "#        for j in res:\n",
    "#            text += str(j)+\" \"\n",
    "\n",
    "#        res = [i for i, val in enumerate(data.rows_[c]) if val]\n",
    "#        text += \"[\"\n",
    "#        for j in res:\n",
    "#            text += str(j)+\" \"\n",
    "#        text += \"]\\n\"\n",
    "    \n",
    "    num_of_clusters = len(co_clusters)\n",
    "    \n",
    "#     for c in range(len(cols)):\n",
    "    for c in range(num_of_clusters):\n",
    "#         for i in cols[c]:\n",
    "        for i in co_clusters[c][0]: # get the attributes in cluster c\n",
    "            text += str(i)+\" \"\n",
    "        \n",
    "        text += \"(\"+str(len(co_clusters[c][1]))+\") [\" # get the number of objects in clusters c\n",
    "        for j in range(len(co_clusters[c][1])): # save in the file each obj\n",
    "            if j+1 != len(co_clusters[c][1]):\n",
    "                text += str(co_clusters[c][1][j])+\" \"\n",
    "            else:\n",
    "                text += str(co_clusters[c][1][j])\n",
    "        text += \"]\\n\"\n",
    "    \n",
    "    #print(text)\n",
    "    if method == 'Dhillon':\n",
    "        f = open('./datasets/outputs/'+fileName+'.txt', 'w+')#saving at dataset folder\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        print(\"Output file saved in: \"+\"./datasets/outputs/\"+fileName+\".txt\")\n",
    "    elif method == 'Kluger':\n",
    "        f = open('./datasets/outputs/'+fileName+'.txt', 'w+')#saving at dataset folder\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        print(\"Output file saved in: \"+\"./datasets/outputs/\"+fileName+\".txt\")\n",
    "    elif method == 'OCoClus':\n",
    "        f = open('./OutputAnalysis/ococlus/'+dataset+'/'+fileName+'.txt', 'w+')#saving at dataset folder\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        print(\"Output file saved in: \"+\"./OutputAnalysis/ococlus/\"+dataset+\"/\"+fileName+\".txt\")\n",
    "    else:\n",
    "        print(\"The output file was not generated. Method option not recognized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rec_error(data,clusters):\n",
    "    '''\n",
    "    This evaluation measure is computed during the algorithm life time.\n",
    "    '''\n",
    "    reconstructed_ococlus = np.zeros(data.shape,dtype=int)\n",
    "    for nc in range(len(clusters)):\n",
    "        for i in clusters[nc][1]: # object cluster\n",
    "            for j in clusters[nc][0]: # attribute cluster\n",
    "                reconstructed_ococlus[int(i)][int(j)] = 1\n",
    "    print(\"Reconstruction error: \",np.sum(np.bitwise_xor(data,reconstructed_ococlus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omega format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clustering_output_omega(co_clusters):\n",
    "# def build_clustering_output_omega(rowClusters,columnClusters):\n",
    "    '''\n",
    "    Build the clustering output format to use in the omega index evaluation from Remy Cazabet version.\n",
    "    It is optional and we just present this version as a complementary information. If you are interested,\n",
    "    check it out on his team work group at https://github.com/isaranto/omega_index.\n",
    "    '''\n",
    "    \n",
    "    num_of_clusters = len(co_clusters)    \n",
    "    clustering = {}\n",
    "    \n",
    "    for nc in range(num_of_clusters):\n",
    "        rowCluster = co_clusters[nc][1]\n",
    "        columnCluster = co_clusters[nc][0]\n",
    "        clustering[\"c\"+str(nc)] = []\n",
    "        \n",
    "        for i in rowCluster:\n",
    "            for j in columnCluster:\n",
    "                clustering[\"c\"+str(nc)].append((\"01\"+str(i)+\"02\"+str(j)))\n",
    "        \n",
    "    return clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eXascale Infolab \n",
    "We used the xmeasure and OvpNMI project that pushished evaluation measures for overlapping task. We can check it on https://github.com/eXascaleInfolab/xmeasures or https://exascale.info/. Look their project on github to know how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmeasures_format(dict_gt):\n",
    "    '''\n",
    "    This function build the xmeasure format to use it on their evaluation measure.\n",
    "    '''\n",
    "    newData = []\n",
    "    for i in range(len(dict_gt)):\n",
    "#         print(dict_gt['c'+str(i)])\n",
    "        stringLine = dict_gt['c'+str(i)][0]\n",
    "        for j in range(1,len(dict_gt['c'+str(i)])):\n",
    "#             stringLine = stringLine+\" \"+dict_gt['c'+str(i)][j]\n",
    "            stringLine += \" \"+dict_gt['c'+str(i)][j]\n",
    "        newData.append(stringLine)\n",
    "    \n",
    "    return newData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
