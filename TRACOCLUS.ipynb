{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from timeit import default_timer as timer\n",
    "import sys, os, shutil, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Version of the packages in this work.'''\n",
    "# # print(\"Pandas version: \",pd.__version__)\n",
    "# print(\"Pandas version: 0.25.1\")\n",
    "# # print(\"Numpy version: \",np.__version__)\n",
    "# print(\"Numpy version: 1.16.4\")\n",
    "# # print(\"Sys python version: \",sys.version)\n",
    "# print(\"Sys python version: 3.7.1 | package by conda-forge [MSC v.1900 64 bit (AMD64)]\")\n",
    "# print(\"IPython: 7.8.0\")\n",
    "# print(\"IPython genutils: 0.2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--->Verbose mode ON.<---\n",
      "Executing TRACOCLUS method\n",
      "Toy example\n",
      "######################################\n",
      "Number of trajectories: 22\n",
      "Number of unique check-ins: 11\n",
      "########################################\n",
      "Map_attribute_to_id:{'hotel': '0', 'trabalho': '1', 'academia': '2', 'restaurante': '3', 'casa': '4', 'farmacia': '5', 'parque': '6', 'estadio': '7', 'padaria': '8', 'festa': '9', 'aeroporto': '10'}\n",
      "\n",
      "Map_id_to_attribute:{'0': 'hotel', '1': 'trabalho', '2': 'academia', '3': 'restaurante', '4': 'casa', '5': 'farmacia', '6': 'parque', '7': 'estadio', '8': 'padaria', '9': 'festa', '10': 'aeroporto'}\n",
      "\n",
      "Frequence_per_poi:{'0': 13, '1': 30, '2': 13, '3': 10, '4': 23, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1, '10': 4}\n",
      "\n",
      "Trajectories: {'0': ['0', '1', '2', '3', '1', '0'], '1': ['4', '1', '2', '3', '4'], '2': ['5', '1', '2', '4'], '3': ['6', '2', '3', '4'], '4': ['5', '2', '3', '7'], '5': ['4', '2', '1', '4'], '6': ['4', '8', '4', '1', '2', '1', '4'], '7': ['4', '1', '2', '1', '4'], '8': ['0', '1', '2', '1', '0'], '9': ['4', '1', '4'], '10': ['4', '1', '9'], '11': ['4', '1', '3', '1', '4'], '12': ['1', '3', '1', '4'], '13': ['0', '1', '0'], '14': ['0', '1', '2', '3', '0'], '15': ['0', '2', '1', '0'], '16': ['10', '1', '0', '10'], '17': ['6', '1', '0'], '18': ['0', '1', '3', '1', '10'], '19': ['4', '1', '3', '1', '10'], '20': ['4', '1', '2', '4', '1', '4'], '21': ['4', '1', '2', '3', '1', '4']}\n",
      "\n",
      "POI occurring at trajectories: {'0': {'15', '16', '13', '8', '18', '17', '0', '14'}, '1': {'2', '7', '21', '16', '9', '5', '13', '18', '1', '11', '19', '0', '20', '8', '6', '10', '15', '17', '12', '14'}, '2': {'15', '4', '20', '2', '21', '3', '5', '8', '6', '1', '7', '0', '14'}, '3': {'4', '21', '3', '18', '1', '11', '19', '0', '12', '14'}, '4': {'10', '20', '2', '21', '3', '9', '5', '6', '1', '11', '7', '19', '12'}, '5': {'4', '2'}, '6': {'17', '3'}, '7': {'4'}, '8': {'6'}, '9': {'10'}, '10': {'19', '16', '18'}}\n",
      "Get data is DONE!\n",
      "\n",
      "Searching co-cluster:  1\n",
      "\n",
      "S:  {'0': 13, '1': 30, '2': 13, '3': 10, '4': 23, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1, '10': 4}\n",
      "Sorted att:  {'1': 30, '4': 23, '0': 13, '2': 13, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "<class 'collections.deque'>\n",
      "s*:  deque([['1', 30], ['4', 23], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "['1', 30]\n",
      "<class 'list'>\n",
      "Head:  1-1\n",
      "Tested head sequence \"1-1\" does NOT exist!\n",
      "Tail:  1-1\n",
      "Tested tail sequence \"1-1\" does NOT exist!\n",
      "\n",
      "Head:  4-1\n",
      "Trajectories with this sequence: {'10', '20', '21', '9', '6', '1', '11', '7', '19'}\n",
      "Num. objs:  9, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Tail:  1-4\n",
      "Trajectories with this sequence: {'20', '21', '9', '5', '6', '11', '7', '12'}\n",
      "Num. objs:  8, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  0-4-1\n",
      "Tested head sequence \"0-4-1\" does NOT exist!\n",
      "Tail:  1-4-0\n",
      "Tested tail sequence \"1-4-0\" does NOT exist!\n",
      "\n",
      "Head:  2-4-1\n",
      "Trajectories with this sequence: {'20'}\n",
      "Num. objs:  1, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Tail:  1-4-2\n",
      "Tested tail sequence \"1-4-2\" does NOT exist!\n",
      "\n",
      "Head:  3-2-4-1\n",
      "Tested head sequence \"3-2-4-1\" does NOT exist!\n",
      "Tail:  1-4-3\n",
      "Tested tail sequence \"1-4-3\" does NOT exist!\n",
      "\n",
      "Head:  10-2-4-1\n",
      "Tested head sequence \"10-2-4-1\" does NOT exist!\n",
      "Tail:  1-4-10\n",
      "Tested tail sequence \"1-4-10\" does NOT exist!\n",
      "\n",
      "Head:  5-2-4-1\n",
      "Tested head sequence \"5-2-4-1\" does NOT exist!\n",
      "Tail:  1-4-5\n",
      "Tested tail sequence \"1-4-5\" does NOT exist!\n",
      "\n",
      "Head:  6-2-4-1\n",
      "Tested head sequence \"6-2-4-1\" does NOT exist!\n",
      "Tail:  1-4-6\n",
      "Tested tail sequence \"1-4-6\" does NOT exist!\n",
      "\n",
      "Head:  7-2-4-1\n",
      "Tested head sequence \"7-2-4-1\" does NOT exist!\n",
      "Tail:  1-4-7\n",
      "Tested tail sequence \"1-4-7\" does NOT exist!\n",
      "\n",
      "Head:  8-2-4-1\n",
      "Tested head sequence \"8-2-4-1\" does NOT exist!\n",
      "Tail:  1-4-8\n",
      "Tested tail sequence \"1-4-8\" does NOT exist!\n",
      "\n",
      "Head:  9-2-4-1\n",
      "Tested head sequence \"9-2-4-1\" does NOT exist!\n",
      "Tail:  1-4-9\n",
      "Tested tail sequence \"1-4-9\" does NOT exist!\n",
      "\n",
      "Head:  1-4\n",
      "Trajectories with this sequence: {'20', '21', '9', '5', '6', '11', '7', '12'}\n",
      "Num. objs:  8, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Tail:  4-1\n",
      "Trajectories with this sequence: {'10', '20', '21', '9', '6', '1', '11', '7', '19'}\n",
      "Num. objs:  9, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  4-1-4\n",
      "Trajectories with this sequence: {'20', '9'}\n",
      "Num. objs:  2, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Tail:  4-1-4\n",
      "Trajectories with this sequence: {'20', '9'}\n",
      "Num. objs:  2, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  0-4-1-4\n",
      "Tested head sequence \"0-4-1-4\" does NOT exist!\n",
      "Tail:  4-1-4-0\n",
      "Tested tail sequence \"4-1-4-0\" does NOT exist!\n",
      "\n",
      "Head:  2-4-1-4\n",
      "Trajectories with this sequence: {'20'}\n",
      "Num. objs:  1, Num. att:  4, Num. covered:  0, Num. noise:  0\n",
      "Tail:  4-1-4-2\n",
      "Tested tail sequence \"4-1-4-2\" does NOT exist!\n",
      "\n",
      "Head:  3-2-4-1-4\n",
      "Tested head sequence \"3-2-4-1-4\" does NOT exist!\n",
      "Tail:  4-1-4-3\n",
      "Tested tail sequence \"4-1-4-3\" does NOT exist!\n",
      "\n",
      "Head:  10-2-4-1-4\n",
      "Tested head sequence \"10-2-4-1-4\" does NOT exist!\n",
      "Tail:  4-1-4-10\n",
      "Tested tail sequence \"4-1-4-10\" does NOT exist!\n",
      "\n",
      "Head:  5-2-4-1-4\n",
      "Tested head sequence \"5-2-4-1-4\" does NOT exist!\n",
      "Tail:  4-1-4-5\n",
      "Tested tail sequence \"4-1-4-5\" does NOT exist!\n",
      "\n",
      "Head:  6-2-4-1-4\n",
      "Tested head sequence \"6-2-4-1-4\" does NOT exist!\n",
      "Tail:  4-1-4-6\n",
      "Tested tail sequence \"4-1-4-6\" does NOT exist!\n",
      "\n",
      "Head:  7-2-4-1-4\n",
      "Tested head sequence \"7-2-4-1-4\" does NOT exist!\n",
      "Tail:  4-1-4-7\n",
      "Tested tail sequence \"4-1-4-7\" does NOT exist!\n",
      "\n",
      "Head:  8-2-4-1-4\n",
      "Tested head sequence \"8-2-4-1-4\" does NOT exist!\n",
      "Tail:  4-1-4-8\n",
      "Tested tail sequence \"4-1-4-8\" does NOT exist!\n",
      "\n",
      "Head:  9-2-4-1-4\n",
      "Tested head sequence \"9-2-4-1-4\" does NOT exist!\n",
      "Tail:  4-1-4-9\n",
      "Tested tail sequence \"4-1-4-9\" does NOT exist!\n",
      "\n",
      "Head:  1-0\n",
      "Trajectories with this sequence: {'15', '16', '13', '8', '17', '0'}\n",
      "Num. objs:  6, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Tail:  0-1\n",
      "Trajectories with this sequence: {'13', '8', '18', '0', '14'}\n",
      "Num. objs:  5, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  4-1-0\n",
      "Tested head sequence \"4-1-0\" does NOT exist!\n",
      "Tail:  0-1-4\n",
      "Tested tail sequence \"0-1-4\" does NOT exist!\n",
      "\n",
      "Head:  0-1-0\n",
      "Trajectories with this sequence: {'13'}\n",
      "Num. objs:  1, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Tail:  0-1-0\n",
      "Trajectories with this sequence: {'13'}\n",
      "Num. objs:  1, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  2-0-1-0\n",
      "Tested head sequence \"2-0-1-0\" does NOT exist!\n",
      "Tail:  0-1-0-2\n",
      "Tested tail sequence \"0-1-0-2\" does NOT exist!\n",
      "\n",
      "Head:  3-0-1-0\n",
      "Tested head sequence \"3-0-1-0\" does NOT exist!\n",
      "Tail:  0-1-0-3\n",
      "Tested tail sequence \"0-1-0-3\" does NOT exist!\n",
      "\n",
      "Head:  10-0-1-0\n",
      "Tested head sequence \"10-0-1-0\" does NOT exist!\n",
      "Tail:  0-1-0-10\n",
      "Tested tail sequence \"0-1-0-10\" does NOT exist!\n",
      "\n",
      "Head:  5-0-1-0\n",
      "Tested head sequence \"5-0-1-0\" does NOT exist!\n",
      "Tail:  0-1-0-5\n",
      "Tested tail sequence \"0-1-0-5\" does NOT exist!\n",
      "\n",
      "Head:  6-0-1-0\n",
      "Tested head sequence \"6-0-1-0\" does NOT exist!\n",
      "Tail:  0-1-0-6\n",
      "Tested tail sequence \"0-1-0-6\" does NOT exist!\n",
      "\n",
      "Head:  7-0-1-0\n",
      "Tested head sequence \"7-0-1-0\" does NOT exist!\n",
      "Tail:  0-1-0-7\n",
      "Tested tail sequence \"0-1-0-7\" does NOT exist!\n",
      "\n",
      "Head:  8-0-1-0\n",
      "Tested head sequence \"8-0-1-0\" does NOT exist!\n",
      "Tail:  0-1-0-8\n",
      "Tested tail sequence \"0-1-0-8\" does NOT exist!\n",
      "\n",
      "Head:  9-0-1-0\n",
      "Tested head sequence \"9-0-1-0\" does NOT exist!\n",
      "Tail:  0-1-0-9\n",
      "Tested tail sequence \"0-1-0-9\" does NOT exist!\n",
      "\n",
      "Head:  1-2\n",
      "Trajectories with this sequence: {'20', '2', '21', '8', '6', '1', '7', '0', '14'}\n",
      "Num. objs:  9, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Tail:  2-1\n",
      "Trajectories with this sequence: {'15', '5', '8', '6', '7'}\n",
      "Num. objs:  5, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  4-1-2\n",
      "Trajectories with this sequence: {'20', '21', '6', '1', '7'}\n",
      "Num. objs:  5, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Tail:  2-1-4\n",
      "Trajectories with this sequence: {'6', '5', '7'}\n",
      "Num. objs:  3, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  0-4-1-2\n",
      "Tested head sequence \"0-4-1-2\" does NOT exist!\n",
      "Tail:  2-1-4-0\n",
      "Tested tail sequence \"2-1-4-0\" does NOT exist!\n",
      "\n",
      "Head:  2-4-1-2\n",
      "Tested head sequence \"2-4-1-2\" does NOT exist!\n",
      "Tail:  2-1-4-2\n",
      "Tested tail sequence \"2-1-4-2\" does NOT exist!\n",
      "\n",
      "Head:  3-4-1-2\n",
      "Tested head sequence \"3-4-1-2\" does NOT exist!\n",
      "Tail:  2-1-4-3\n",
      "Tested tail sequence \"2-1-4-3\" does NOT exist!\n",
      "\n",
      "Head:  10-4-1-2\n",
      "Tested head sequence \"10-4-1-2\" does NOT exist!\n",
      "Tail:  2-1-4-10\n",
      "Tested tail sequence \"2-1-4-10\" does NOT exist!\n",
      "\n",
      "Head:  5-4-1-2\n",
      "Tested head sequence \"5-4-1-2\" does NOT exist!\n",
      "Tail:  2-1-4-5\n",
      "Tested tail sequence \"2-1-4-5\" does NOT exist!\n",
      "\n",
      "Head:  6-4-1-2\n",
      "Tested head sequence \"6-4-1-2\" does NOT exist!\n",
      "Tail:  2-1-4-6\n",
      "Tested tail sequence \"2-1-4-6\" does NOT exist!\n",
      "\n",
      "Head:  7-4-1-2\n",
      "Tested head sequence \"7-4-1-2\" does NOT exist!\n",
      "Tail:  2-1-4-7\n",
      "Tested tail sequence \"2-1-4-7\" does NOT exist!\n",
      "\n",
      "Head:  8-4-1-2\n",
      "Trajectories with this sequence: {'6'}\n",
      "Num. objs:  1, Num. att:  4, Num. covered:  0, Num. noise:  0\n",
      "Tail:  2-1-4-8\n",
      "Tested tail sequence \"2-1-4-8\" does NOT exist!\n",
      "\n",
      "Head:  9-8-4-1-2\n",
      "Tested head sequence \"9-8-4-1-2\" does NOT exist!\n",
      "Tail:  2-1-4-9\n",
      "Tested tail sequence \"2-1-4-9\" does NOT exist!\n",
      "\n",
      "Head:  1-3\n",
      "Trajectories with this sequence: {'19', '18', '12', '11'}\n",
      "Num. objs:  4, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Tail:  3-1\n",
      "Trajectories with this sequence: {'21', '18', '11', '19', '0', '12'}\n",
      "Num. objs:  6, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  4-1-3\n",
      "Trajectories with this sequence: {'19', '11'}\n",
      "Num. objs:  2, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Tail:  3-1-4\n",
      "Trajectories with this sequence: {'11', '12', '21'}\n",
      "Num. objs:  3, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  0-4-1-3\n",
      "Tested head sequence \"0-4-1-3\" does NOT exist!\n",
      "Tail:  3-1-4-0\n",
      "Tested tail sequence \"3-1-4-0\" does NOT exist!\n",
      "\n",
      "Head:  2-4-1-3\n",
      "Tested head sequence \"2-4-1-3\" does NOT exist!\n",
      "Tail:  3-1-4-2\n",
      "Tested tail sequence \"3-1-4-2\" does NOT exist!\n",
      "\n",
      "Head:  3-4-1-3\n",
      "Tested head sequence \"3-4-1-3\" does NOT exist!\n",
      "Tail:  3-1-4-3\n",
      "Tested tail sequence \"3-1-4-3\" does NOT exist!\n",
      "\n",
      "Head:  10-4-1-3\n",
      "Tested head sequence \"10-4-1-3\" does NOT exist!\n",
      "Tail:  3-1-4-10\n",
      "Tested tail sequence \"3-1-4-10\" does NOT exist!\n",
      "\n",
      "Head:  5-4-1-3\n",
      "Tested head sequence \"5-4-1-3\" does NOT exist!\n",
      "Tail:  3-1-4-5\n",
      "Tested tail sequence \"3-1-4-5\" does NOT exist!\n",
      "\n",
      "Head:  6-4-1-3\n",
      "Tested head sequence \"6-4-1-3\" does NOT exist!\n",
      "Tail:  3-1-4-6\n",
      "Tested tail sequence \"3-1-4-6\" does NOT exist!\n",
      "\n",
      "Head:  7-4-1-3\n",
      "Tested head sequence \"7-4-1-3\" does NOT exist!\n",
      "Tail:  3-1-4-7\n",
      "Tested tail sequence \"3-1-4-7\" does NOT exist!\n",
      "\n",
      "Head:  8-4-1-3\n",
      "Tested head sequence \"8-4-1-3\" does NOT exist!\n",
      "Tail:  3-1-4-8\n",
      "Tested tail sequence \"3-1-4-8\" does NOT exist!\n",
      "\n",
      "Head:  9-4-1-3\n",
      "Tested head sequence \"9-4-1-3\" does NOT exist!\n",
      "Tail:  3-1-4-9\n",
      "Tested tail sequence \"3-1-4-9\" does NOT exist!\n",
      "\n",
      "Head:  1-10\n",
      "Trajectories with this sequence: {'19', '18'}\n",
      "Num. objs:  2, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Tail:  10-1\n",
      "Trajectories with this sequence: {'16'}\n",
      "Num. objs:  1, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  4-1-10\n",
      "Tested head sequence \"4-1-10\" does NOT exist!\n",
      "Tail:  10-1-4\n",
      "Tested tail sequence \"10-1-4\" does NOT exist!\n",
      "\n",
      "Head:  0-1-10\n",
      "Tested head sequence \"0-1-10\" does NOT exist!\n",
      "Tail:  10-1-0\n",
      "Trajectories with this sequence: {'16'}\n",
      "Num. objs:  1, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  2-1-10\n",
      "Tested head sequence \"2-1-10\" does NOT exist!\n",
      "Tail:  10-1-0-2\n",
      "Tested tail sequence \"10-1-0-2\" does NOT exist!\n",
      "\n",
      "Head:  3-1-10\n",
      "Trajectories with this sequence: {'19', '18'}\n",
      "Num. objs:  2, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Tail:  10-1-0-3\n",
      "Tested tail sequence \"10-1-0-3\" does NOT exist!\n",
      "\n",
      "Head:  10-3-1-10\n",
      "Tested head sequence \"10-3-1-10\" does NOT exist!\n",
      "Tail:  10-1-0-10\n",
      "Trajectories with this sequence: {'16'}\n",
      "Num. objs:  1, Num. att:  4, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  5-3-1-10\n",
      "Tested head sequence \"5-3-1-10\" does NOT exist!\n",
      "Tail:  10-1-0-10-5\n",
      "Tested tail sequence \"10-1-0-10-5\" does NOT exist!\n",
      "\n",
      "Head:  6-3-1-10\n",
      "Tested head sequence \"6-3-1-10\" does NOT exist!\n",
      "Tail:  10-1-0-10-6\n",
      "Tested tail sequence \"10-1-0-10-6\" does NOT exist!\n",
      "\n",
      "Head:  7-3-1-10\n",
      "Tested head sequence \"7-3-1-10\" does NOT exist!\n",
      "Tail:  10-1-0-10-7\n",
      "Tested tail sequence \"10-1-0-10-7\" does NOT exist!\n",
      "\n",
      "Head:  8-3-1-10\n",
      "Tested head sequence \"8-3-1-10\" does NOT exist!\n",
      "Tail:  10-1-0-10-8\n",
      "Tested tail sequence \"10-1-0-10-8\" does NOT exist!\n",
      "\n",
      "Head:  9-3-1-10\n",
      "Tested head sequence \"9-3-1-10\" does NOT exist!\n",
      "Tail:  10-1-0-10-9\n",
      "Tested tail sequence \"10-1-0-10-9\" does NOT exist!\n",
      "\n",
      "Head:  1-5\n",
      "Tested head sequence \"1-5\" does NOT exist!\n",
      "Tail:  5-1\n",
      "Trajectories with this sequence: {'2'}\n",
      "Num. objs:  1, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  4-5\n",
      "Tested head sequence \"4-5\" does NOT exist!\n",
      "Tail:  5-1-4\n",
      "Tested tail sequence \"5-1-4\" does NOT exist!\n",
      "\n",
      "Head:  0-5\n",
      "Tested head sequence \"0-5\" does NOT exist!\n",
      "Tail:  5-1-0\n",
      "Tested tail sequence \"5-1-0\" does NOT exist!\n",
      "\n",
      "Head:  2-5\n",
      "Tested head sequence \"2-5\" does NOT exist!\n",
      "Tail:  5-1-2\n",
      "Trajectories with this sequence: {'2'}\n",
      "Num. objs:  1, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  3-5\n",
      "Tested head sequence \"3-5\" does NOT exist!\n",
      "Tail:  5-1-2-3\n",
      "Tested tail sequence \"5-1-2-3\" does NOT exist!\n",
      "\n",
      "Head:  10-5\n",
      "Tested head sequence \"10-5\" does NOT exist!\n",
      "Tail:  5-1-2-10\n",
      "Tested tail sequence \"5-1-2-10\" does NOT exist!\n",
      "\n",
      "Head:  5-5\n",
      "Tested head sequence \"5-5\" does NOT exist!\n",
      "Tail:  5-1-2-5\n",
      "Tested tail sequence \"5-1-2-5\" does NOT exist!\n",
      "\n",
      "Head:  6-5\n",
      "Tested head sequence \"6-5\" does NOT exist!\n",
      "Tail:  5-1-2-6\n",
      "Tested tail sequence \"5-1-2-6\" does NOT exist!\n",
      "\n",
      "Head:  7-5\n",
      "Tested head sequence \"7-5\" does NOT exist!\n",
      "Tail:  5-1-2-7\n",
      "Tested tail sequence \"5-1-2-7\" does NOT exist!\n",
      "\n",
      "Head:  8-5\n",
      "Tested head sequence \"8-5\" does NOT exist!\n",
      "Tail:  5-1-2-8\n",
      "Tested tail sequence \"5-1-2-8\" does NOT exist!\n",
      "\n",
      "Head:  9-5\n",
      "Tested head sequence \"9-5\" does NOT exist!\n",
      "Tail:  5-1-2-9\n",
      "Tested tail sequence \"5-1-2-9\" does NOT exist!\n",
      "\n",
      "Head:  1-6\n",
      "Tested head sequence \"1-6\" does NOT exist!\n",
      "Tail:  6-1\n",
      "Trajectories with this sequence: {'17'}\n",
      "Num. objs:  1, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  4-6\n",
      "Tested head sequence \"4-6\" does NOT exist!\n",
      "Tail:  6-1-4\n",
      "Tested tail sequence \"6-1-4\" does NOT exist!\n",
      "\n",
      "Head:  0-6\n",
      "Tested head sequence \"0-6\" does NOT exist!\n",
      "Tail:  6-1-0\n",
      "Trajectories with this sequence: {'17'}\n",
      "Num. objs:  1, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  2-6\n",
      "Tested head sequence \"2-6\" does NOT exist!\n",
      "Tail:  6-1-0-2\n",
      "Tested tail sequence \"6-1-0-2\" does NOT exist!\n",
      "\n",
      "Head:  3-6\n",
      "Tested head sequence \"3-6\" does NOT exist!\n",
      "Tail:  6-1-0-3\n",
      "Tested tail sequence \"6-1-0-3\" does NOT exist!\n",
      "\n",
      "Head:  10-6\n",
      "Tested head sequence \"10-6\" does NOT exist!\n",
      "Tail:  6-1-0-10\n",
      "Tested tail sequence \"6-1-0-10\" does NOT exist!\n",
      "\n",
      "Head:  5-6\n",
      "Tested head sequence \"5-6\" does NOT exist!\n",
      "Tail:  6-1-0-5\n",
      "Tested tail sequence \"6-1-0-5\" does NOT exist!\n",
      "\n",
      "Head:  6-6\n",
      "Tested head sequence \"6-6\" does NOT exist!\n",
      "Tail:  6-1-0-6\n",
      "Tested tail sequence \"6-1-0-6\" does NOT exist!\n",
      "\n",
      "Head:  7-6\n",
      "Tested head sequence \"7-6\" does NOT exist!\n",
      "Tail:  6-1-0-7\n",
      "Tested tail sequence \"6-1-0-7\" does NOT exist!\n",
      "\n",
      "Head:  8-6\n",
      "Tested head sequence \"8-6\" does NOT exist!\n",
      "Tail:  6-1-0-8\n",
      "Tested tail sequence \"6-1-0-8\" does NOT exist!\n",
      "\n",
      "Head:  9-6\n",
      "Tested head sequence \"9-6\" does NOT exist!\n",
      "Tail:  6-1-0-9\n",
      "Tested tail sequence \"6-1-0-9\" does NOT exist!\n",
      "\n",
      "Head:  1-7\n",
      "Tested head sequence \"1-7\" does NOT exist!\n",
      "Tail:  7-1\n",
      "Tested tail sequence \"7-1\" does NOT exist!\n",
      "\n",
      "Head:  4-7\n",
      "Tested head sequence \"4-7\" does NOT exist!\n",
      "Tail:  7-4\n",
      "Tested tail sequence \"7-4\" does NOT exist!\n",
      "\n",
      "Head:  0-7\n",
      "Tested head sequence \"0-7\" does NOT exist!\n",
      "Tail:  7-0\n",
      "Tested tail sequence \"7-0\" does NOT exist!\n",
      "\n",
      "Head:  2-7\n",
      "Tested head sequence \"2-7\" does NOT exist!\n",
      "Tail:  7-2\n",
      "Tested tail sequence \"7-2\" does NOT exist!\n",
      "\n",
      "Head:  3-7\n",
      "Trajectories with this sequence: {'4'}\n",
      "Num. objs:  1, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Tail:  7-3\n",
      "Tested tail sequence \"7-3\" does NOT exist!\n",
      "\n",
      "Head:  10-3-7\n",
      "Tested head sequence \"10-3-7\" does NOT exist!\n",
      "Tail:  7-10\n",
      "Tested tail sequence \"7-10\" does NOT exist!\n",
      "\n",
      "Head:  5-3-7\n",
      "Tested head sequence \"5-3-7\" does NOT exist!\n",
      "Tail:  7-5\n",
      "Tested tail sequence \"7-5\" does NOT exist!\n",
      "\n",
      "Head:  6-3-7\n",
      "Tested head sequence \"6-3-7\" does NOT exist!\n",
      "Tail:  7-6\n",
      "Tested tail sequence \"7-6\" does NOT exist!\n",
      "\n",
      "Head:  7-3-7\n",
      "Tested head sequence \"7-3-7\" does NOT exist!\n",
      "Tail:  7-7\n",
      "Tested tail sequence \"7-7\" does NOT exist!\n",
      "\n",
      "Head:  8-3-7\n",
      "Tested head sequence \"8-3-7\" does NOT exist!\n",
      "Tail:  7-8\n",
      "Tested tail sequence \"7-8\" does NOT exist!\n",
      "\n",
      "Head:  9-3-7\n",
      "Tested head sequence \"9-3-7\" does NOT exist!\n",
      "Tail:  7-9\n",
      "Tested tail sequence \"7-9\" does NOT exist!\n",
      "\n",
      "Head:  1-8\n",
      "Tested head sequence \"1-8\" does NOT exist!\n",
      "Tail:  8-1\n",
      "Tested tail sequence \"8-1\" does NOT exist!\n",
      "\n",
      "Head:  4-8\n",
      "Trajectories with this sequence: {'6'}\n",
      "Num. objs:  1, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Tail:  8-4\n",
      "Trajectories with this sequence: {'6'}\n",
      "Num. objs:  1, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "\n",
      "Head:  0-4-8\n",
      "Tested head sequence \"0-4-8\" does NOT exist!\n",
      "Tail:  8-4-0\n",
      "Tested tail sequence \"8-4-0\" does NOT exist!\n",
      "\n",
      "Head:  2-4-8\n",
      "Tested head sequence \"2-4-8\" does NOT exist!\n",
      "Tail:  8-4-2\n",
      "Tested tail sequence \"8-4-2\" does NOT exist!\n",
      "\n",
      "Head:  3-4-8\n",
      "Tested head sequence \"3-4-8\" does NOT exist!\n",
      "Tail:  8-4-3\n",
      "Tested tail sequence \"8-4-3\" does NOT exist!\n",
      "\n",
      "Head:  10-4-8\n",
      "Tested head sequence \"10-4-8\" does NOT exist!\n",
      "Tail:  8-4-10\n",
      "Tested tail sequence \"8-4-10\" does NOT exist!\n",
      "\n",
      "Head:  5-4-8\n",
      "Tested head sequence \"5-4-8\" does NOT exist!\n",
      "Tail:  8-4-5\n",
      "Tested tail sequence \"8-4-5\" does NOT exist!\n",
      "\n",
      "Head:  6-4-8\n",
      "Tested head sequence \"6-4-8\" does NOT exist!\n",
      "Tail:  8-4-6\n",
      "Tested tail sequence \"8-4-6\" does NOT exist!\n",
      "\n",
      "Head:  7-4-8\n",
      "Tested head sequence \"7-4-8\" does NOT exist!\n",
      "Tail:  8-4-7\n",
      "Tested tail sequence \"8-4-7\" does NOT exist!\n",
      "\n",
      "Head:  8-4-8\n",
      "Tested head sequence \"8-4-8\" does NOT exist!\n",
      "Tail:  8-4-8\n",
      "Tested tail sequence \"8-4-8\" does NOT exist!\n",
      "\n",
      "Head:  9-4-8\n",
      "Tested head sequence \"9-4-8\" does NOT exist!\n",
      "Tail:  8-4-9\n",
      "Tested tail sequence \"8-4-9\" does NOT exist!\n",
      "\n",
      "Head:  1-9\n",
      "Trajectories with this sequence: {'10'}\n",
      "Num. objs:  1, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Tail:  9-1\n",
      "Tested tail sequence \"9-1\" does NOT exist!\n",
      "\n",
      "Head:  4-1-9\n",
      "Trajectories with this sequence: {'10'}\n",
      "Num. objs:  1, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Tail:  9-4\n",
      "Tested tail sequence \"9-4\" does NOT exist!\n",
      "\n",
      "Head:  0-4-1-9\n",
      "Tested head sequence \"0-4-1-9\" does NOT exist!\n",
      "Tail:  9-0\n",
      "Tested tail sequence \"9-0\" does NOT exist!\n",
      "\n",
      "Head:  2-4-1-9\n",
      "Tested head sequence \"2-4-1-9\" does NOT exist!\n",
      "Tail:  9-2\n",
      "Tested tail sequence \"9-2\" does NOT exist!\n",
      "\n",
      "Head:  3-4-1-9\n",
      "Tested head sequence \"3-4-1-9\" does NOT exist!\n",
      "Tail:  9-3\n",
      "Tested tail sequence \"9-3\" does NOT exist!\n",
      "\n",
      "Head:  10-4-1-9\n",
      "Tested head sequence \"10-4-1-9\" does NOT exist!\n",
      "Tail:  9-10\n",
      "Tested tail sequence \"9-10\" does NOT exist!\n",
      "\n",
      "Head:  5-4-1-9\n",
      "Tested head sequence \"5-4-1-9\" does NOT exist!\n",
      "Tail:  9-5\n",
      "Tested tail sequence \"9-5\" does NOT exist!\n",
      "\n",
      "Head:  6-4-1-9\n",
      "Tested head sequence \"6-4-1-9\" does NOT exist!\n",
      "Tail:  9-6\n",
      "Tested tail sequence \"9-6\" does NOT exist!\n",
      "\n",
      "Head:  7-4-1-9\n",
      "Tested head sequence \"7-4-1-9\" does NOT exist!\n",
      "Tail:  9-7\n",
      "Tested tail sequence \"9-7\" does NOT exist!\n",
      "\n",
      "Head:  8-4-1-9\n",
      "Tested head sequence \"8-4-1-9\" does NOT exist!\n",
      "Tail:  9-8\n",
      "Tested tail sequence \"9-8\" does NOT exist!\n",
      "\n",
      "Head:  9-4-1-9\n",
      "Tested head sequence \"9-4-1-9\" does NOT exist!\n",
      "Tail:  9-9\n",
      "Tested tail sequence \"9-9\" does NOT exist!\n",
      "\n",
      "0.07374540000000707\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#toy example dataset\n",
    "# input_test2.dat and input_test2.dat\n",
    "\n",
    "#synthetic datasets\n",
    "# synthetic-1_fimi.dat, synthetic-2_fimi.dat and synthetic-3_fimi.dat\n",
    "\n",
    "experiment = 'Toy' # Syn: run the synthetic data analysis; Real: run the real data analysis; \n",
    "                   # Toy: run the toy example.\n",
    "toy_number = \"2\" # sufix of a given toy dataset [1,1-1,2]\n",
    "\n",
    "find_overlap = False # True: find overlapped co-clusters; False: Find no overlapped co-clusters\n",
    "VERBOSE = True # True: print results as a verbose mode; False: print the main results\n",
    "num_of_sim = 1 # number of simulations to perform\n",
    "k = -1 #max number of co-cluster that could be found. -1: default [driven by cost function]\n",
    "e_obj = 0.4 # maximum error tolerance for object. -1: default [accept the maximum error]\n",
    "e_att = .8 # maximum error tolerance for attribute. -1: default [accept the maximum error]\n",
    "\n",
    "if VERBOSE: print('--->Verbose mode ON.<---')\n",
    "\n",
    "print(\"Executing TRACOCLUS method\")\n",
    "if experiment == 'Syn':\n",
    "    path = \"./data/synthetic/fimi/\"\n",
    "    syn_datasets = [path+\"synthetic-1_fimi.dat\",path+\"synthetic-2_fimi.dat\",path+\"synthetic-3_fimi.dat\"]\n",
    "    path_method = \"OutputAnalysis\\ococlus\"\n",
    "    check_path(path_method)\n",
    "\n",
    "    for ds in range(len(syn_datasets)):\n",
    "        ds_name = \"Syn-\"+str(ds+1)\n",
    "        print(\"\\nDataset: \"+ds_name)\n",
    "        res = os.mkdir(path_method+\"\\\\\"+ds_name)\n",
    "\n",
    "        for run in range(num_of_sim):\n",
    "            print(\"Run-\"+str(run+1))\n",
    "\n",
    "            df_fimi = pd.read_csv(syn_datasets[ds], header=None, names=[\"transation\"])\n",
    "            D,co_clusters = OCoClus(df_fimi,k,e_obj,e_att) # Calling OCoClus main method\n",
    "\n",
    "            print(\"\")\n",
    "            Rec_error(D,co_clusters)\n",
    "#             print(co_clusters)\n",
    "            \n",
    "            omega_format = build_clustering_output_omega(co_clusters)\n",
    "            OCoClus_clustering_xm = xmeasures_format(omega_format)# save to XMEASURES format C++ version\n",
    "            df_gt = pd.DataFrame(OCoClus_clustering_xm)\n",
    "            path = path_method+\"/\"+ds_name\n",
    "            df_gt.to_csv(path.replace(\"\\\\\",\"/\")+\"/run_\"+str(run+1)+\"_res_ococlus_\"+ds_name+\"_co.cnl\", \n",
    "                         header= False,index=False, encoding='utf8')\n",
    "            del omega_format, df_gt, OCoClus_clustering_xm\n",
    "            gc.collect()\n",
    "elif experiment == 'Real':\n",
    "    k = 10\n",
    "    print('Real data clustering.')\n",
    "    path = \"./data/real_application/\"\n",
    "    real_datasets = [path+\"cal500_fimi.dat\",path+\"covid19_fimi.dat\"]\n",
    "    path_method = \"OutputAnalysis\\ococlus\"\n",
    "    check_path(path_method)\n",
    "    \n",
    "    for ds in range(len(real_datasets)):\n",
    "        ds_name = real_datasets[ds].replace(\"/\",\"_\").split(\"_\")[4].capitalize()\n",
    "        print(\"\\nDataset: \"+ds_name)\n",
    "        res = os.mkdir(path_method+\"\\\\\"+ds_name)\n",
    "        \n",
    "        df_fimi = pd.read_csv(real_datasets[ds], header=None, names=[\"transation\"])\n",
    "        D,co_clusters = OCoClus(df_fimi,k,e_obj,e_att) # Calling OCoClus main method\n",
    "        writeFileOutput(co_clusters,ds_name,method='OCoClus',fileName='OCoClusResult_'+ds_name)\n",
    "        \n",
    "    print('DONE!')\n",
    "elif experiment == 'Toy':\n",
    "    print('Toy example')\n",
    "#     input_data_pd = pd.read_csv(\"./data/toy_example/toy\"+toy_number+\"_traj.dat\", header=None, names=[\"transation\"])\n",
    "    input_data_pd = pd.read_csv(\"./data/toy_example/toy\"+toy_number+\"_traj.dat\", header=None, names=[\"transation\"])\n",
    "#     print(input_data_pd)\n",
    "    D,co_clusters = TRACOCLUS(input_data_pd,k,e_obj,e_att)\n",
    "    print(co_clusters)\n",
    "    #Compute the measures\n",
    "#     print(\"\")\n",
    "#     Rec_error(D,co_clusters)\n",
    "else:\n",
    "    print('ERROR! Choose a valid option for the experiment analysis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Omega index, overlapped F1, ONMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "### omega index and f-score\n",
    "### ./xmeasures -o -fp -ku -O gt.txt cls2.txt\n",
    "# pwd\n",
    "# ls\n",
    "\n",
    "if [ -d \"xmeasures/OutputAnalysis/ococlus\" ] \n",
    "then\n",
    "#     echo \"Directory exists.\"\n",
    "    rm -R xmeasures/OutputAnalysis/ococlus\n",
    "else\n",
    "    echo \"Error: Directory does not exists.\"\n",
    "fi\n",
    "\n",
    "cp -R OutputAnalysis xmeasures\n",
    "cd xmeasures/\n",
    "\n",
    "#Method: [Ococlus]\n",
    "#ground-truth: [gt_xm_s1_co.cnl,gt_xm_s2_co.cnl,gt_xm_s3_co.cnl]\n",
    "for i in 1 2 3 # index of synthetic datasets\n",
    "# for i in 1\n",
    "do\n",
    "#     for file in OutputAnalysis/ococlus/Syn-${i}/*\n",
    "    for file in OutputAnalysis/ococlus/Syn-${i}/*\n",
    "    do\n",
    "    #     echo \" $(grep -c '' ${file})\"\n",
    "        res=$(grep -c '' ${file})\n",
    "    #     echo \"${file} lines equal to: ${res}\"\n",
    "\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "    #         echo \"Empty file\"\n",
    "            ./xmeasures -o -fp -ku -O ./gts/gt_xm_s${i}_trad.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "#     # wait until all child processes are done\n",
    "#     wait\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "#### ONMI\n",
    "#### ./onmi file1 file2\n",
    "\n",
    "cd xmeasures\n",
    "\n",
    "#Method: [Ococlus]\n",
    "#ground-truth: [gt_xm_s1_trad.cnl,gt_xm_s2_trad.cnl,gt_xm_s3_trad.cnl]\n",
    "for i in 1 2 3 # index of synthetic datasets\n",
    "# for i in 1\n",
    "do\n",
    "    for file in OutputAnalysis/ococlus/Syn-${i}/*\n",
    "    do\n",
    "    #     echo \" $(grep -c '' ${file})\"\n",
    "        res=$(grep -c '' ${file})\n",
    "    #     echo \"${file} lines equal to: ${res}\"\n",
    "\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "    #         echo \"Empty file\"\n",
    "            ./onmi ./gts/gt_xm_s${i}_trad.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "#     # wait until all child processes are done\n",
    "#     wait\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TraCoClus algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main algorithm of TraCoClus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '3', '1', '4', '0', '1']\n"
     ]
    }
   ],
   "source": [
    "er = '0-3-1-4-0-1'\n",
    "er2 = er.split('-')\n",
    "print(er2)\n",
    "# print(er2.nunique())\n",
    "def change_vect_cont(queue):\n",
    "    queue[0] = 'Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q', 2, 8, 4]\n"
     ]
    }
   ],
   "source": [
    "my_vect = [1,2,8,4]\n",
    "change_vect_cont(my_vect)\n",
    "print(my_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################\n",
      "Number of trajectories: 22\n",
      "Number of unique check-ins: 11\n",
      "########################################\n",
      "Map_attribute_to_id:{'hotel': '0', 'trabalho': '1', 'academia': '2', 'restaurante': '3', 'casa': '4', 'farmacia': '5', 'parque': '6', 'estadio': '7', 'padaria': '8', 'festa': '9', 'aeroporto': '10'}\n",
      "\n",
      "Map_id_to_attribute:{'0': 'hotel', '1': 'trabalho', '2': 'academia', '3': 'restaurante', '4': 'casa', '5': 'farmacia', '6': 'parque', '7': 'estadio', '8': 'padaria', '9': 'festa', '10': 'aeroporto'}\n",
      "\n",
      "Frequence_per_poi:{'0': 13, '1': 30, '2': 13, '3': 10, '4': 23, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1, '10': 4}\n",
      "\n",
      "Trajectories: {'0': ['0', '1', '2', '3', '1', '0'], '1': ['4', '1', '2', '3', '4'], '2': ['5', '1', '2', '4'], '3': ['6', '2', '3', '4'], '4': ['5', '2', '3', '7'], '5': ['4', '2', '1', '4'], '6': ['4', '8', '4', '1', '2', '1', '4'], '7': ['4', '1', '2', '1', '4'], '8': ['0', '1', '2', '1', '0'], '9': ['4', '1', '4'], '10': ['4', '1', '9'], '11': ['4', '1', '3', '1', '4'], '12': ['1', '3', '1', '4'], '13': ['0', '1', '0'], '14': ['0', '1', '2', '3', '0'], '15': ['0', '2', '1', '0'], '16': ['10', '1', '0', '10'], '17': ['6', '1', '0'], '18': ['0', '1', '3', '1', '10'], '19': ['4', '1', '3', '1', '10'], '20': ['4', '1', '2', '4', '1', '4'], '21': ['4', '1', '2', '3', '1', '4']}\n",
      "\n",
      "POI occurring at trajectories: {'0': {'15', '16', '13', '8', '18', '17', '0', '14'}, '1': {'2', '7', '21', '16', '9', '5', '13', '18', '1', '11', '19', '0', '20', '8', '6', '10', '15', '17', '12', '14'}, '2': {'15', '4', '20', '2', '21', '3', '5', '8', '6', '1', '7', '0', '14'}, '3': {'4', '21', '3', '18', '1', '11', '19', '0', '12', '14'}, '4': {'10', '20', '2', '21', '3', '9', '5', '6', '1', '11', '7', '19', '12'}, '5': {'4', '2'}, '6': {'17', '3'}, '7': {'4'}, '8': {'6'}, '9': {'10'}, '10': {'19', '16', '18'}}\n",
      "Get data is DONE!\n",
      "\n",
      "Searching co-cluster:  1\n",
      "\n",
      "S:  {'0': 13, '1': 30, '2': 13, '3': 10, '4': 23, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1, '10': 4}\n",
      "Sorted att:  {'1': 30, '4': 23, '0': 13, '2': 13, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "<class 'collections.deque'>\n",
      "s*:  deque([['1', 30], ['4', 23], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "['1', 30]\n",
      "<class 'list'>\n",
      "\n",
      "Head sequence:  1-1\n",
      "Tested head sequence \"1-1\" does NOT exist!\n",
      "Tail sequence:  1-1\n",
      "Tested tail sequence \"1-1\" does NOT exist!\n",
      "Current co-cluster cost:  9223372036854775807\n",
      "Queue s* BEFORE to upadate:  deque([['1', 30], ['4', 23], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 30], ['4', 23], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  4-1\n",
      "Trajectories with this sequence: {'10', '20', '21', '9', '6', '1', '11', '7', '19'}\n",
      "Num. objs:  9, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Head cost: -7\n",
      "Tail sequence:  1-4\n",
      "Trajectories with this sequence: {'20', '21', '9', '5', '6', '11', '7', '12'}\n",
      "Num. objs:  8, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Tail cost: -6\n",
      "Current co-cluster cost:  9223372036854775807\n",
      "Queue s* BEFORE to upadate:  deque([['1', 30], ['4', 23], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Co-cluster improved with HEAD sequence.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  0-4-1\n",
      "Tested head sequence \"0-4-1\" does NOT exist!\n",
      "Tail sequence:  4-1-0\n",
      "Tested tail sequence \"4-1-0\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  2-4-1\n",
      "Trajectories with this sequence: {'20'}\n",
      "Num. objs:  1, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Head cost: 1\n",
      "Tail sequence:  4-1-2\n",
      "Trajectories with this sequence: {'20', '21', '6', '1', '7'}\n",
      "Num. objs:  5, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Tail cost: -7\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Co-cluster improved with TAIL sequence.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  3-4-1-2\n",
      "Tested head sequence \"3-4-1-2\" does NOT exist!\n",
      "Tail sequence:  4-1-2-3\n",
      "Trajectories with this sequence: {'1', '21'}\n",
      "Num. objs:  2, Num. att:  4, Num. covered:  0, Num. noise:  0\n",
      "Tail cost: -2\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  10-4-1-2\n",
      "Tested head sequence \"10-4-1-2\" does NOT exist!\n",
      "Tail sequence:  4-1-2-10\n",
      "Tested tail sequence \"4-1-2-10\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  5-4-1-2\n",
      "Tested head sequence \"5-4-1-2\" does NOT exist!\n",
      "Tail sequence:  4-1-2-5\n",
      "Tested tail sequence \"4-1-2-5\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  6-4-1-2\n",
      "Tested head sequence \"6-4-1-2\" does NOT exist!\n",
      "Tail sequence:  4-1-2-6\n",
      "Tested tail sequence \"4-1-2-6\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  7-4-1-2\n",
      "Tested head sequence \"7-4-1-2\" does NOT exist!\n",
      "Tail sequence:  4-1-2-7\n",
      "Tested tail sequence \"4-1-2-7\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  8-4-1-2\n",
      "Trajectories with this sequence: {'6'}\n",
      "Num. objs:  1, Num. att:  4, Num. covered:  0, Num. noise:  0\n",
      "Head cost: 1\n",
      "Tail sequence:  4-1-2-8\n",
      "Tested tail sequence \"4-1-2-8\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  9-4-1-2\n",
      "Tested head sequence \"9-4-1-2\" does NOT exist!\n",
      "Tail sequence:  4-1-2-9\n",
      "Tested tail sequence \"4-1-2-9\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Co-cluster identified. Go to the next searching.\n",
      "Main list S BEFORE to update:  {'1': 30, '4': 23, '0': 13, '2': 13, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "Main list S AFTER to update:  {'1': 30, '4': 23, '0': 13, '2': 13, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "Cluster \"1\" finished at time \"0.00856440000643488\".\n",
      "Total clustering time:  0.00858810001227539\n"
     ]
    }
   ],
   "source": [
    "D,co_clusters = TRACOCLUS(input_data_pd,k,e_obj,e_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_queue(poi_freq_dict):\n",
    "#     print([{k:v} for k,v in poi_freq_dict.items()])\n",
    "    queue = deque()\n",
    "    [queue.append([k,v]) for k,v in poi_freq_dict.items()]\n",
    "    return queue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sequence(trajectory_dataset_dict_list, candidate_trajectories_sequence_set, test_traj_sequence):\n",
    "    '''This method receive 3 parameters: \n",
    "       1) trajectory dataset as a dict->list | x['key']:[...];\n",
    "       2) trajectories indeces that contains the tested check-ins as a set;\n",
    "       3) the given tested sequence of check-ins as a string\n",
    "    '''\n",
    "    new_set_trajectories = set()\n",
    "    position_pois_per_traj_list = {}\n",
    "#     test_traj_sequence = test_traj_sequence.strip()\n",
    "    for traj_id in candidate_trajectories_sequence_set:\n",
    "\n",
    "        try:\n",
    "#             traj_dataset = '-'.join(trajectory_dataset_dict_list[traj_id]).strip()\n",
    "            test_subsequence, positions_at_traj = is_subsequence(trajectory_dataset_dict_list[traj_id],test_traj_sequence.split('-'))\n",
    "            if test_subsequence:\n",
    "#                 print('OK->',end=' ')\n",
    "#                 print(traj_id,positions_at_traj)\n",
    "                new_set_trajectories.add(traj_id)\n",
    "                position_pois_per_traj_list[traj_id] = positions_at_traj\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "#         print('{}-> {}'.format(traj_id,trajectory_dataset_dict_list[traj_id]))\n",
    "#     print('Sequence \"{}\" is present in trajectories: {}'.format(test_traj_sequence,new_set_trajectories))\n",
    "    return new_set_trajectories, position_pois_per_traj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_subsequence(sequence, subsequence):\n",
    "    '''This sub method receive two arrays: \n",
    "       first one is the sequence and second one is the tested subsequence.'''\n",
    "    n = len(sequence)\n",
    "    m = len(subsequence)\n",
    "    position_poi_sequence = []\n",
    "    \n",
    "    # Two pointers to traverse the arrays\n",
    "    i = 0; j = 0;\n",
    " \n",
    "    # Traverse both arrays simultaneously\n",
    "    while (i < n and j < m):\n",
    " \n",
    "        # If element matches\n",
    "        # increment both pointers\n",
    "        if (sequence[i] == subsequence[j]):\n",
    "            position_poi_sequence.append(str(i))\n",
    "            i += 1\n",
    "            j += 1\n",
    " \n",
    "            # If array B is completely\n",
    "            # traversed\n",
    "            if (j == m):\n",
    "                return True, position_poi_sequence\n",
    "         \n",
    "        # If not,\n",
    "        # increment i and reset j\n",
    "        else:\n",
    "            position_poi_sequence = []\n",
    "            i = i - j + 1\n",
    "            j = 0\n",
    "         \n",
    "    return False,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ['2', '3']\n",
      "0 ['3', '4']\n",
      "['013', '0104', '312', '3103']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0104', '013', '3103', '312'}"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = {'0':['1','4','6','1','10'],'1':['3','6','7'],'2':['7','9','5'],'3':['4','6','1','10']\n",
    "                ,'4':['9','5']}\n",
    "test_candidate = set(['0','1','2','3','4'])\n",
    "test_sequence = '1-10'\n",
    "mySet, myPos = check_sequence(test_dataset,test_candidate,test_sequence)\n",
    "print([rows+test_sequence.split('-')[poi_id]+poi_pos for rows in mySet \n",
    " for poi_id in range(len(test_sequence.split('-'))) for poi_pos in myPos[rows][poi_id]])\n",
    "form_elements(mySet,test_sequence,myPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_elements(trajs_index_set, tested_sequence, poi_positions_trajectories_dict_list):\n",
    "    '''\n",
    "    This method returns a set of elements.\n",
    "    Each element is formed by the traj ID, poi ID in the sequence and its respective position at traj ID.\n",
    "    Ex: set(['013', '0104', '312', '3103'])\n",
    "        '013': 0-> traj ID, 1-> poi ID, and 3-> position of poi ID at traj ID\n",
    "    '''\n",
    "    tested_sequence = tested_sequence.split('-')\n",
    "    return set([trajID+tested_sequence[poi_id]+poi_pos for trajID in trajs_index_set \n",
    "                for poi_id in range(len(tested_sequence)) \n",
    "                for poi_pos in poi_positions_trajectories_dict_list[trajID][poi_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def TRACOCLUS(input_D, k=-1, e_obj=-1, e_att=-1):\n",
    "    ### variable declaration\n",
    "    if k == -1:\n",
    "        k=sys.maxsize\n",
    "    if e_obj == -1:\n",
    "        e_obj = 1\n",
    "    if e_att == -1:\n",
    "        e_att = 1\n",
    "    \n",
    "    cost_model = sys.float_info.max # initial cost function of the model\n",
    "    num_of_coclusters = 0\n",
    "    D = []\n",
    "    final_coclusters = [] # store the attribute and objects clusters. final_coclusters[[C1_att,C1_obj],[Ck_att,Ck_obj]]\n",
    "    pattern_model = [set(),set()]# Union between the found co-clusters [list of obj,list of att]\n",
    "    cost_per_cocluster = []# stores the cost to build the cocluster\n",
    "    history_cost_model = []\n",
    "    ###\n",
    "    \n",
    "#     D,N,data_dict,data_res_dict,map_id_to_attribute = get_data(input_D)\n",
    "    map_id_to_attribute_dict, S_poi_freq_dict, poi_at_trajs_dict_set, trajs_data_dict_list = get_data(input_D)\n",
    "    \n",
    "    final_coclusters = []\n",
    "    final_clustered_elements = set()\n",
    "    \n",
    "    start = timer()\n",
    "    for iter_k in range(1):\n",
    "        print('\\nSearching co-cluster: ',iter_k+1)\n",
    "        print('')\n",
    "        print('S: ',S_poi_freq_dict)\n",
    "        S_poi_freq_dict = sort_attributes(S_poi_freq_dict)\n",
    "        s_poi_freq_queue_list = populate_queue(S_poi_freq_dict)\n",
    "        print(type(s_poi_freq_queue_list))\n",
    "        print('s*: ',s_poi_freq_queue_list)\n",
    "        print(s_poi_freq_queue_list[0])\n",
    "        print(type(s_poi_freq_queue_list[0]))\n",
    "        print('')\n",
    "        \n",
    "        cocluster_sequence_str = ''\n",
    "        cocluster_attributes_list = ''\n",
    "        cocluster_index_rows_set = set()\n",
    "        cocluster_elements_set = set()\n",
    "        cocluster_cost_function = sys.maxsize\n",
    "        \n",
    "        for poi_name, poi_frequence in S_poi_freq_dict.items():\n",
    "#             print(poi_name,poi_frequence)\n",
    "            if cocluster_sequence_str == '':\n",
    "                head_sequence_str = poi_name\n",
    "                trajectories_head_sequence_set = poi_at_trajs_dict_set[poi_name]\n",
    "                tail_sequence_str = poi_name\n",
    "                trajectories_tail_sequence_set = poi_at_trajs_dict_set[poi_name]\n",
    "\n",
    "                for poi_node_queue in s_poi_freq_queue_list:\n",
    "    #                 print(poi_node_queue)\n",
    "    #                 print(poi_node_queue[0],poi_node_queue[1])\n",
    "\n",
    "                    #### test POI at the head ###\n",
    "                    tmp_head_sequence_str = head_sequence_str\n",
    "                    head_sequence_str = poi_node_queue[0]+'-'+head_sequence_str\n",
    "                    print('Head sequence: ',head_sequence_str)\n",
    "                    tmp_traj_set = trajectories_head_sequence_set\n",
    "                    trajectories_head_sequence_set = trajectories_head_sequence_set.intersection(poi_at_trajs_dict_set[poi_node_queue[0]])\n",
    "                    trajectories_head_sequence_set, position_poi_per_traj_head = check_sequence(trajs_data_dict_list,\n",
    "                                                                                                trajectories_head_sequence_set,\n",
    "                                                                                                head_sequence_str)\n",
    "                    \n",
    "                    if len(trajectories_head_sequence_set) > 0:\n",
    "                        print('Trajectories with this sequence: {}'.format(trajectories_head_sequence_set))\n",
    "                        elements_head_sequence = form_elements(trajectories_head_sequence_set,\n",
    "                                                               head_sequence_str,\n",
    "                                                               position_poi_per_traj_head)    \n",
    "                        overlapped_elements = elements_head_sequence.intersection(final_clustered_elements)\n",
    "                        cost_head_sequence = cost_function(len(trajectories_head_sequence_set),\n",
    "                                                           len(head_sequence_str.split('-')),\n",
    "                                                           len(overlapped_elements))\n",
    "                        print('Head cost: {}'.format(cost_head_sequence))\n",
    "                    else:\n",
    "                        print('Tested head sequence \"{}\" does NOT exist!'.format(head_sequence_str))\n",
    "                        trajectories_head_sequence_set = tmp_traj_set\n",
    "                        head_sequence_str = tmp_head_sequence_str\n",
    "                        cost_head_sequence = sys.maxsize\n",
    "                    #### END test HEAD sequence ####\n",
    "\n",
    "                    #### test POI at the TAIL ####\n",
    "                    tmp_tail_sequence_str = tail_sequence_str\n",
    "                    tail_sequence_str = tail_sequence_str+'-'+poi_node_queue[0]\n",
    "                    print('Tail sequence: ',tail_sequence_str)\n",
    "                    tmp_traj_set = trajectories_tail_sequence_set\n",
    "                    trajectories_tail_sequence_set = trajectories_tail_sequence_set.intersection(poi_at_trajs_dict_set[poi_node_queue[0]])\n",
    "                    trajectories_tail_sequence_set, position_poi_per_traj_tail = check_sequence(trajs_data_dict_list,\n",
    "                                                                                                trajectories_tail_sequence_set,\n",
    "                                                                                                tail_sequence_str)\n",
    "                    \n",
    "                    if (len(trajectories_tail_sequence_set) > 0):\n",
    "                        print('Trajectories with this sequence: {}'.format(trajectories_tail_sequence_set))\n",
    "                        elements_tail_sequence = form_elements(trajectories_tail_sequence_set,\n",
    "                                                               tail_sequence_str,\n",
    "                                                               position_poi_per_traj_tail)\n",
    "                        overlapped_elements = elements_tail_sequence.intersection(final_clustered_elements)\n",
    "                        cost_tail_sequence = cost_function(len(trajectories_tail_sequence_set),\n",
    "                                                           len(tail_sequence_str.split('-')),\n",
    "                                                           len(overlapped_elements))\n",
    "                        print('Tail cost: {}'.format(cost_tail_sequence))\n",
    "                    else:\n",
    "                        print('Tested tail sequence \"{}\" does NOT exist!'.format(tail_sequence_str))\n",
    "                        trajectories_tail_sequence_set = tmp_traj_set\n",
    "                        tail_sequence_str = tmp_tail_sequence_str\n",
    "                        cost_tail_sequence = sys.maxsize\n",
    "                    #### END test TAIL sequence ####\n",
    "\n",
    "\n",
    "                    print('Current co-cluster cost: ',cocluster_cost_function)\n",
    "                    print('Queue s* BEFORE to upadate: ',s_poi_freq_queue_list)\n",
    "                    \n",
    "                    ### test if some sequence can improve the current co-cluster\n",
    "                    improved_cocluster = False\n",
    "                    if ((cost_head_sequence < cost_tail_sequence) and \n",
    "                        (cost_head_sequence <= cocluster_cost_function)):\n",
    "                        print('Co-cluster improved with HEAD sequence.')\n",
    "                        \n",
    "                        # update the nodes of queue s.\n",
    "                        update_queue_s(cocluster_sequence_str,head_sequence_str,\n",
    "                                       s_poi_freq_queue_list,poi_node_queue)\n",
    "                        \n",
    "                        cocluster_sequence_str = head_sequence_str\n",
    "                        cocluster_attributes_list = head_sequence_str.split('-')\n",
    "                        cocluster_index_rows_set = trajectories_head_sequence_set.copy()\n",
    "                        cocluster_elements_set = elements_head_sequence.copy()\n",
    "                        cocluster_cost_function = cost_head_sequence\n",
    "                        improved_cocluster = True\n",
    "                    elif ((cost_tail_sequence < cost_head_sequence) and \n",
    "                          (cost_tail_sequence <= cocluster_cost_function)):\n",
    "                        print('Co-cluster improved with TAIL sequence.')\n",
    "                        \n",
    "                        # update the nodes of queue s.\n",
    "                        update_queue_s(cocluster_sequence_str,tail_sequence_str,\n",
    "                                       s_poi_freq_queue_list,poi_node_queue)\n",
    "                        \n",
    "                        cocluster_sequence_str = tail_sequence_str\n",
    "                        cocluster_attributes_list = tail_sequence_str.split('-')\n",
    "                        cocluster_index_rows_set = trajectories_tail_sequence_set.copy()\n",
    "                        cocluster_elements_set = elements_tail_sequence.copy()\n",
    "                        cocluster_cost_function = cost_tail_sequence\n",
    "                        improved_cocluster = True\n",
    "                    \n",
    "                    if improved_cocluster:\n",
    "                        trajectories_head_sequence_set = cocluster_index_rows_set\n",
    "                        head_sequence_str = cocluster_sequence_str\n",
    "                        trajectories_tail_sequence_set = cocluster_index_rows_set\n",
    "                        tail_sequence_str = cocluster_sequence_str\n",
    "#                         ## decrementar attributo em s\n",
    "#                         print('Poi node: ', poi_node_queue)\n",
    "#                         poi_node_queue[1] -= 1\n",
    "#                         print('Poi node: ', poi_node_queue)\n",
    "                    else:\n",
    "                        print('Tested sequences do not improved the current co-cluster.')\n",
    "                        trajectories_head_sequence_set = tmp_traj_set\n",
    "                        head_sequence_str = tmp_head_sequence_str\n",
    "                        trajectories_tail_sequence_set = tmp_traj_set\n",
    "                        tail_sequence_str = tmp_tail_sequence_str\n",
    "                    \n",
    "                    print('Queue s* AFTER to update: ',s_poi_freq_queue_list)\n",
    "\n",
    "                    print('')\n",
    "    #                 tail_sequence_str = tail_sequence_str+'-'+poi_node_queue[0]\n",
    "    #                 print('Tail: ',tail_sequence_str)\n",
    "            \n",
    "            else:\n",
    "                print('Co-cluster identified. Go to the next searching.')\n",
    "                print('Main list S BEFORE to update: ',S_poi_freq_dict)\n",
    "                print('Main list S AFTER to update: ',S_poi_freq_dict)\n",
    "#                 partial = timer()\n",
    "#                 print('Cluster \"{}\" finished at time \"{}\".'.format((iter_k+1),(partial-start))\n",
    "                break\n",
    "            \n",
    "            \n",
    "        ## into loop of iteration k\n",
    "        partial = timer()\n",
    "        print('Cluster \"{}\" finished at time \"{}\".'.format(iter_k+1,partial-start))\n",
    "    ## out of loop iteraton k\n",
    "    end = timer()\n",
    "    print('Total clustering time: ',str(end-start))\n",
    "        # candidate_cocluster = find_base_cocluster()\n",
    "        # max_cocluster = expand_cocluster()\n",
    "        # testa se max_cocluster != VAZIO e ainda no descoberto\n",
    "            # OK: salva max_cocluster no conjunto de co-clusters\n",
    "            #     atualiza a lista de frequencia S\n",
    "            # FAIL: terminar busca por novos co-clusters\n",
    "    \n",
    "    # return conjunto de co-clusters\n",
    "            \n",
    "    \n",
    "    \n",
    "#     print(\"Data dict (in main ococlus):\"+str(data_dict))\n",
    "#     for itertator in range(1,k+1):\n",
    "#         print(\"Iterator: \",itertator)\n",
    "#         if VERBOSE:\n",
    "#             print(\"Pattern model: \",pattern_model)\n",
    "#         C,E,new_cost_dense = find_dense_cocluster(data_dict, data_res_dict)\n",
    "#         if new_cost_dense >= 0:\n",
    "#             print(\"No relevant co-cluster can be found anymore.\")\n",
    "#             break    \n",
    "#         C_expanded = expand_dense_cocluster(C, E, data_dict,e_obj,e_att)\n",
    "#         print(\"Expanded: \"+str(C_expanded))\n",
    "\n",
    "#         print(\"\")\n",
    "#         new_cost = cost_function(len(pattern_model[0].union(C[1])),\n",
    "#                                               len(pattern_model[1].union(set(C[0]))))\n",
    "#         if VERBOSE:\n",
    "#             print(\"Dense co-cluster cost: \",new_cost_dense)\n",
    "#             print(\"Current model cost: \",cost_model)\n",
    "#             print(\"New model cost with the found co-cluster: \",new_cost)\n",
    "#             print(\"Attribute cluster:\"+str(C[0])+\", Object cluster: \"+str(C[1]))\n",
    "#             print(\" \")\n",
    "        \n",
    "#         if new_cost < cost_model:\n",
    "#             final_coclusters.append(C)\n",
    "#             cost_model = new_cost\n",
    "#             num_of_coclusters += 1\n",
    "#             data_res_dict = update_residual_dataset(data_res_dict,C[0],C[1])\n",
    "#             pattern_model[0] = pattern_model[0].union(C[1])\n",
    "#             pattern_model[1] = pattern_model[1].union(set(C[0]))\n",
    "#         else:\n",
    "#             print(\"No co-cluster can be found anymore.\")\n",
    "#             break\n",
    "    \n",
    "#     print(\"Find global co-cluster is done.\")\n",
    "    \n",
    "#     if find_overlap:\n",
    "#         attributes_per_cluster = []\n",
    "#         objects_per_cluster = []\n",
    "#         for i in range(len(final_coclusters)):\n",
    "#             attributes_per_cluster.append(set(list(map(int,final_coclusters[i][0]))))\n",
    "#             objects_per_cluster.append(final_coclusters[i][1])\n",
    "\n",
    "#         print(\"\\nFind derived overlapped co-clusters method\")\n",
    "#         attribute_clusters, objects_clusters = findOverlap(attributes_per_cluster,objects_per_cluster)\n",
    "\n",
    "#         final_coclusters = []\n",
    "#         for i in range(len(attribute_clusters)):\n",
    "#             final_coclusters.append([attribute_clusters[i],objects_clusters[i]])\n",
    "\n",
    "#         print(\"\\nNon-overlapped and overlapped co-clusters.\")\n",
    "#         print(\"Number of co-clusters: \",len(final_coclusters))\n",
    "#         if VERBOSE:\n",
    "#             print(\"Final co-clusters: \",final_coclusters)\n",
    "#     else:\n",
    "#         print(\"\\nNon-overlapped co-clusters.\")\n",
    "#         print(\"Number of co-clusters: \",num_of_coclusters)\n",
    "# #         for i in range(len(final_coclusters)):\n",
    "# #             tmp_data = final_coclusters[i][1]\n",
    "# #             final_coclusters[i][1] = list(tmp_data)\n",
    "#         if VERBOSE:\n",
    "#             print(\"Final co-clusters: \",final_coclusters)\n",
    "    \n",
    "#     print(\"Final co-clusters:\"+str(final_coclusters))\n",
    "    \n",
    "#     print(\"Remap the attributes to its original label (final co-clusters):\")\n",
    "#     # go back to the original att map\n",
    "#     for i in range(len(final_coclusters)):\n",
    "#         cluster_att_id = final_coclusters[i][0]\n",
    "#         for j in range(len(cluster_att_id)):\n",
    "#             final_coclusters[i][0][j] = map_id_to_attribute[str(cluster_att_id[j])]\n",
    "            \n",
    "    return D,final_coclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_queue_s(cocluster_sequence_str,tested_sequence_str,s_poi_freq_queue_list,poi_node_queue):\n",
    "    '''\n",
    "    Method to update the nodes in queue s. It decrements the value of a given node in s.\n",
    "    The input are:\n",
    "        1. The current string sequence of a cocluster;\n",
    "        2. The tested string sequence to improve a cocluster;\n",
    "        3. The queue s;\n",
    "        4. A single node of queue s.\n",
    "    '''\n",
    "    # update list s when the first sequence is identified\n",
    "    if cocluster_sequence_str == '':\n",
    "        tmp_split = tested_sequence_str.split('-')\n",
    "        for attribute in tmp_split:\n",
    "            for node_s in s_poi_freq_queue_list:\n",
    "                if attribute == node_s[0]:\n",
    "                    node_s[1] -= 1\n",
    "                    break\n",
    "    else: # update a single node in case a sequence is already discovered\n",
    "        poi_node_queue[1] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a1',\n",
       " 'a2',\n",
       " 'a3',\n",
       " 'a4',\n",
       " 'b1',\n",
       " 'b2',\n",
       " 'b3',\n",
       " 'b4',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'c3',\n",
       " 'c4',\n",
       " 'd1',\n",
       " 'd2',\n",
       " 'd3',\n",
       " 'd4']"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = ['a','b','c','d']\n",
    "a2 = [1,2,3,4]\n",
    "[str(i)+str(j) for i in a1 for j in a2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 3 0 0 0 0]\n",
      " [1 1 3 0 0 0 0]\n",
      " [1 1 3 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "[[1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]]\n",
      "[[2 2 4 1 1 1 1]\n",
      " [2 2 4 1 1 1 1]\n",
      " [2 2 4 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]]\n",
      "[[4 4 4 4 4 4 4]\n",
      " [4 4 4 4 4 4 4]\n",
      " [4 4 4 4 4 4 4]\n",
      " [4 4 4 4 4 4 4]\n",
      " [4 4 4 4 4 4 4]\n",
      " [4 4 4 4 4 4 4]\n",
      " [4 4 4 4 4 4 4]]\n",
      "[[-3 -3 -3 -3 -3 -3 -3]\n",
      " [-3 -3 -3 -3 -3 -3 -3]\n",
      " [-3 -3 -3 -3 -3 -3 -3]\n",
      " [-3 -3 -3 -3 -3 -3 -3]\n",
      " [-3 -3 -3 -3 -3 -3 -3]\n",
      " [-3 -3 -3 -3 -3 -3 -3]\n",
      " [-3 -3 -3 -3 -3 -3 -3]]\n",
      "-147\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,1,1,0,0,0,0])\n",
    "b = np.array([1,1,3,0,0,0,0])\n",
    "c = np.outer(a,b)\n",
    "print(c)\n",
    "d = (c*0)+1\n",
    "print(d)\n",
    "print(c+d)\n",
    "e = d*4\n",
    "print(e)\n",
    "f = d-e\n",
    "print(f)\n",
    "print(sum(sum(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['f', 2], ['h', 5], ['t', 1]]\n",
      "[['f', 2], ['h', 5], ['t', 1]]\n",
      "[['h', 5], ['f', 2], ['t', 1]]\n",
      "[['h', 4], ['f', 2], ['t', 1]]\n"
     ]
    }
   ],
   "source": [
    "def myFunc(e):\n",
    "    return e[:][1]\n",
    "\n",
    "er = [['f',2],['h',5],['t',1]]\n",
    "print(er)\n",
    "er.sort()\n",
    "print(er)\n",
    "er.sort(reverse=True, key=myFunc)\n",
    "print(er)\n",
    "er[0][1] -= 1\n",
    "print(er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find dense co-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dense_cocluster(input_dataset, residual_dataset):\n",
    "    '''\n",
    "    Input\n",
    "        input_dataset: a dictionary with the list of objects per attribute. input_dataset[att] -> [objects_in_att]\n",
    "        residual_dataset: it is a copy of input_dataset used to sort the attributes\n",
    "    \n",
    "    Output\n",
    "        C: A dense co-cluster. It is a array with a list of attributes and a set of objects. C[att_list,obj_set]\n",
    "    '''\n",
    "    print(\"Find dense cocluster method.\")\n",
    "    att_dense_cocluster_list = []\n",
    "#     att_dense_cocluster_set = set([])\n",
    "    obj_dense_cocluster_list = []\n",
    "    att_extension_list = []\n",
    "    cost_dense_cocluster = sys.float_info.max\n",
    "    \n",
    "    sads = sort_att_ds(residual_dataset)\n",
    "#     print(\"Sorted att: \",sads)\n",
    "    cc_att = sads[0]\n",
    "#     att_dense_cocluster_list.append(cc_att)\n",
    "    att_dense_cocluster_list.append(cc_att)\n",
    "    obj_dense_cocluter_set = set(residual_dataset[cc_att]) # get the objs forthe given attribute cc_att\n",
    "    count_group_att = 1\n",
    "#     cost_dense_cocluster = cost_function(len(pattern_model[0].union(obj_dense_cocluter_set)),\n",
    "#                                          len(pattern_model[1].union(set(cc_att))))\n",
    "    new_cost_function = cost_function(len(obj_dense_cocluter_set), count_group_att)\n",
    "    if VERBOSE:\n",
    "        print(\"New cost: \"+str(new_cost_function)+\", Cost dense: \"+str(cost_dense_cocluster))\n",
    "    cost_dense_cocluster = new_cost_function\n",
    "    \n",
    "    for next_att in range(1,len(sads)):\n",
    "        cc_att_test = sads[next_att]\n",
    "#         print(\"Attribute: \",cc_att_test)\n",
    "        curr_cc_att = set(residual_dataset[cc_att_test])\n",
    "        intersection_objs = obj_dense_cocluter_set.intersection(curr_cc_att)\n",
    "#         print(\"Intersection test: \",intersection_objs)\n",
    "        tmp = att_dense_cocluster_list.copy()\n",
    "        tmp.append(cc_att_test)\n",
    "#         new_cost_function = cost_function(len(pattern_model[0].union(intersection_objs)),\n",
    "#                                           len(pattern_model[1].union(set(tmp))))\n",
    "        new_cost_function = cost_function(len(intersection_objs),count_group_att+1)\n",
    "#         print(intersection_objs,curr_cc_att)\n",
    "#         print(len(intersection_objs),count_group_att+1)\n",
    "        if VERBOSE:\n",
    "            print(\"New cost: \"+str(new_cost_function)+\", Cost dense: \"+str(cost_dense_cocluster))\n",
    "            \n",
    "        if  new_cost_function <= cost_dense_cocluster:\n",
    "            att_dense_cocluster_list.append(cc_att_test)\n",
    "            obj_dense_cocluter_set = intersection_objs\n",
    "            cost_dense_cocluster = new_cost_function\n",
    "            count_group_att += 1\n",
    "        else:\n",
    "            att_extension_list.append(cc_att_test)\n",
    "    \n",
    "    C = [att_dense_cocluster_list, obj_dense_cocluter_set]\n",
    "    print(att_dense_cocluster_list)\n",
    "    \n",
    "    # no good rectangle was found\n",
    "    if cost_dense_cocluster >= 0:\n",
    "        att_extension_list = []\n",
    "        C = [[],set()]\n",
    "    else:\n",
    "        C = [att_dense_cocluster_list, obj_dense_cocluter_set]\n",
    "    \n",
    "    return C, att_extension_list, cost_dense_cocluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= [1,2,3,4,5,6,7,8]\n",
    "print(t)\n",
    "push_to_end = 3\n",
    "complete_cicle = False\n",
    "reload = True\n",
    "print(t.pop(2))\n",
    "print(t)\n",
    "t= [1,2,3,4,5,6,7,8]\n",
    "print(t)\n",
    "# while(reload and complete_cicle != True):\n",
    "#     for i in range(len(t)):\n",
    "#         if t[i] == push_to_end:\n",
    "#             tmp = t.pop(i)\n",
    "#             t.append(tmp)\n",
    "#             complete_cicle == True\n",
    "#         else:\n",
    "#             print(t[i])\n",
    "#         if push_to_end == t[i] and complete_cicle == True:\n",
    "#             reload = False\n",
    "#             break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand dense co-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dense_cocluster(C,E,att_data_dict,e_obj,e_att):\n",
    "    '''\n",
    "    INPUT\n",
    "        C = a tuple(list_atts, set_objs); list_atts has the list of attributes and set_objs is a set of objects\n",
    "        E = list of attributes not present in C_a to be tested\n",
    "        att_data_dict = a dict with list of objects per att; att_data_dict[att] -> [objects]\n",
    "        e_obj = maximum object error \n",
    "        e_att = maximum attribute error \n",
    "    \n",
    "    OUTPUT\n",
    "        \n",
    "    '''\n",
    "    print(\"Expand co-cluster method.\")\n",
    "    added_att = True\n",
    "    \n",
    "    if not C[0]: # nothing good to discover\n",
    "        pass\n",
    "    else:\n",
    "        curr_cost = cost_function(len(C[1]), len(C[0]),0,0)\n",
    "        noise_added = 0 # the quantanty of noise added in the cluster during the process\n",
    "        while(added_att):\n",
    "            # try to add new objects to cocluster C\n",
    "            try_new_objs = set(np.arange(0,len(D))).difference(C[1]) # get the objects not present in C\n",
    "            if VERBOSE:\n",
    "                print(\"# Try to extend the list of Objects #\")\n",
    "            for obj in try_new_objs:\n",
    "                obj_quantanty = 0\n",
    "                for att in C[0]:\n",
    "                    if D[obj][int(att)] == 1:\n",
    "                        obj_quantanty += 1\n",
    "                if not_too_noisy(obj_quantanty, C, e_obj, e_att, att_data_dict, E, \"obj\"):\n",
    "#                     print(\"Rudo valor: \",not_noise_val)\n",
    "                    if VERBOSE:\n",
    "                        print(\"OBJ->\"+str(obj),end=\" | \")\n",
    "                    new_cost = cost_function(len(C[1])+1,len(C[0]),0,(noise_added+(len(C[0])-obj_quantanty)))\n",
    "                    if new_cost <= curr_cost:\n",
    "                        if VERBOSE:\n",
    "                            print(\"New_cost:\"+str(new_cost)+\" ; Curr_cost:\"+str(curr_cost))\n",
    "                        \n",
    "                        C[1].add(obj)\n",
    "                        curr_cost = new_cost\n",
    "                        noise_added += (len(C[0])-obj_quantanty)\n",
    "                else:\n",
    "                    if VERBOSE:\n",
    "                        print(\"Object too noisy to be added. (Obj: \"+str(obj)+\")\")\n",
    "\n",
    "            added_att = False\n",
    "            # try to add new attributes to cocluster C\n",
    "            if VERBOSE:\n",
    "                print(\"# Try to extend the list of Attributes #\")\n",
    "            \n",
    "            while(len(E) != 0):\n",
    "                att = E.pop(0)\n",
    "                if VERBOSE:\n",
    "                    print(\"ATT->\"+str(att), end= \" | \")\n",
    "                att_obj_quantanty = len(C[1].intersection(set(att_data_dict[att])))\n",
    "                if not_too_noisy(att_obj_quantanty, C, e_obj, e_att, att_data_dict, E, \"att\"):\n",
    "                    new_cost = cost_function(len(C[1]),len(C[0])+1,0,(noise_added+(len(C[1])-att_obj_quantanty)))\n",
    "                    \n",
    "                    if VERBOSE:\n",
    "                        print(\"New_cost:\"+str(new_cost)+\" ; Curr_cost:\"+str(curr_cost))\n",
    "                    \n",
    "                    if new_cost <= curr_cost:\n",
    "                        C[0].append(str(att))\n",
    "                        curr_cost = new_cost\n",
    "                        added_att = True\n",
    "                        noise_added += (len(C[1])-att_obj_quantanty)\n",
    "                        break\n",
    "                else:\n",
    "                    if VERBOSE:\n",
    "                        print(\"Attribute too noisy to be added. (Att: \"+str(att)+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find overlapped co-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findOverlap(SetsC,SetsR):\n",
    "    '''\n",
    "    INPUT\n",
    "        SetsC: It has K sets of attributes regarding each attribute cluster\n",
    "        SetsR: It has K sets of objects regarding each object cluster\n",
    "    \n",
    "    OUTPUT\n",
    "        columnClusters: A list with the attribute clusters\n",
    "        rowClusters: A list with the object clusters\n",
    "    '''\n",
    "    newSetsColumns = []\n",
    "    newSetsRows = []\n",
    "    \n",
    "    # merge sets that can overlap\n",
    "    merge(SetsC,SetsR,newSetsColumns,newSetsRows)\n",
    "\n",
    "    #Removing sets with redundant information\n",
    "    removeSubsets(newSetsColumns,newSetsRows)\n",
    "    \n",
    "    columnClusters = newSetsColumns\n",
    "    rowClusters = newSetsRows\n",
    "    return columnClusters,rowClusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support functions for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(input_data):\n",
    "    '''\n",
    "    This method will assign the variables used by the algorithm.\n",
    "    \n",
    "    INPUT\n",
    "        input_data: A panda dataframe of the input data file.\n",
    "    \n",
    "    OUTPUT\n",
    "        D: A binary matrix from the input data.\n",
    "        N: A noise binary matrix with the same size of D.\n",
    "        data_dict: A dictionary to store D as a vertical representation.\n",
    "        data_res_dict: A copy of data_dict used to sort the attributes of D and find unconvered elements.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    data_pd = input_data #txt file with sequence of check-ins (POI)\n",
    "    frequence_per_poi_dict = {} # store the frequence of a POI as \"POI\": num_of_occurrences\n",
    "    poi_at_trajs_dict_set = {}  # store a set with each index line (tid trajectory) that contains a given POI.\n",
    "                            # \"POI\": set(0,1,4,...); It is the S variable\n",
    "#     global data_res_dict\n",
    "    uncover_poi_dict = {} # It is the s* variable\n",
    "#     global D # input data as a binary matrix\n",
    "#     global N # noise matrix with the same size of D\n",
    "    num_of_objects = 0\n",
    "    num_of_attributes = 0\n",
    "    map_id_to_attribute = {} # map the \n",
    "    map_attribute_to_id = {} # map the\n",
    "    trajectory_dict = {} # it stores the trajectories with its check-ins. \"TID\": [POI1,POI2,...]\n",
    "#     max_val_att = 0 \n",
    "    att_id = 0 # assign an ID to each attribute\n",
    "    \n",
    "    # read each line\n",
    "    for index, row in data_pd.iterrows():\n",
    "        num_of_objects+=1\n",
    "        object_data = row[0].split(\" \")\n",
    "#         trajectory_dict[str(index)] = {}\n",
    "#         trajectory_dict[str(index)] = object_data\n",
    "        trajectory_dict[str(index)] = []\n",
    "        \n",
    "#         for attribute in object_data: # we look at each item of the given transaction\n",
    "        for att_j in range(len(object_data)): # we look at each item of the given transaction\n",
    "            attribute = object_data[att_j]\n",
    "            \n",
    "            if attribute != \"\":\n",
    "#                 if int(attribute) > max_val_att:\n",
    "#                     max_val_att = int(attribute)\n",
    "#                 if attribute not in map_unique_attributes_dataset:\n",
    "#                 if attribute not in map_attribute_to_id.keys():\n",
    "    \n",
    "                if attribute not in map_attribute_to_id: # mapping\n",
    "#                     unique_attributes_dataset.append(attribute)\n",
    "                    map_attribute_to_id[attribute] = str(att_id)\n",
    "                    map_id_to_attribute[str(att_id)] = attribute\n",
    "                    att_id += 1\n",
    "                \n",
    "                # substitute the check-in by its ID\n",
    "                trajectory_dict[str(index)].append(map_attribute_to_id[attribute])\n",
    "                \n",
    "                # store the indeces containing a given POI\n",
    "                if map_attribute_to_id[attribute] in poi_at_trajs_dict_set:\n",
    "#                     data_dict[map_attribute_to_id[attribute]].append(index)\n",
    "                    poi_at_trajs_dict_set[map_attribute_to_id[attribute]].add(str(index))\n",
    "                else:\n",
    "#                     data_dict[map_attribute_to_id[attribute]] = [index]\n",
    "                    poi_at_trajs_dict_set[map_attribute_to_id[attribute]] = set([str(index)])\n",
    "                \n",
    "                # store the frequence for each POI\n",
    "                if map_attribute_to_id[attribute] in frequence_per_poi_dict:\n",
    "                    current_value = frequence_per_poi_dict[map_attribute_to_id[attribute]]\n",
    "                    frequence_per_poi_dict[map_attribute_to_id[attribute]] = current_value + 1\n",
    "                else:\n",
    "                    frequence_per_poi_dict[map_attribute_to_id[attribute]] = 1\n",
    "            \n",
    "#             # simulate the idea of a linked list structure with a dict to facility the check\n",
    "#             if att_j == 0 and len(object_data) > 1:\n",
    "# #                 print(\"IF 1\")\n",
    "# #                 print(object_data[att_j])\n",
    "#                 trajectory_dict[str(index)][map_attribute_to_id[attribute]] = {\"Previous\": \"None\", \"Next\": \"None\"}\n",
    "#             elif att_j == 0 and len(object_data) == 1:\n",
    "# #                 print(\"IF 2\")\n",
    "# #                 print(object_data[att_j])\n",
    "#                 trajectory_dict[str(index)][map_attribute_to_id[attribute]] = {\"Previous\": \"None\", \"Next\": \"None\"}\n",
    "#             elif att_j > 0 and att_j < (len(object_data)-1):\n",
    "# #                 print(\"IF 3\")\n",
    "# #                 print(object_data[att_j])\n",
    "#                 trajectory_dict[str(index)][map_attribute_to_id[attribute]] = {\"Previous\": map_attribute_to_id[object_data[att_j-1]],\n",
    "#                                                                                \"Next\": \"None\"}\n",
    "#                 trajectory_dict[str(index)][map_attribute_to_id[object_data[att_j-1]]][\"Next\"] = map_attribute_to_id[attribute]\n",
    "#             else:\n",
    "# #                 print(\"IF 4\")\n",
    "# #                 print(object_data[att_j])\n",
    "#                 trajectory_dict[str(index)][map_attribute_to_id[attribute]] = {\"Previous\": map_attribute_to_id[object_data[att_j-1]],\n",
    "#                                                                                \"Next\": \"None\"}\n",
    "#                 trajectory_dict[str(index)][map_attribute_to_id[object_data[att_j-1]]][\"Next\"] = map_attribute_to_id[attribute]\n",
    "            \n",
    "        \n",
    "#     print(\"Trajectory dict:\"+str(trajectory_dict))\n",
    "                    \n",
    "    uncover_poi_dict = poi_at_trajs_dict_set.copy()\n",
    "#     num_of_attributes = len(data_dict)\n",
    "#     num_of_attributes = max_val_att+1\n",
    "#     num_of_attributes = len(map_attribute_to_id)\n",
    "    print(\"######################################\")\n",
    "    print(\"Number of trajectories: \"+str(index+1))\n",
    "    print(\"Number of unique check-ins: \"+str(len(map_attribute_to_id)))\n",
    "    print(\"########################################\")\n",
    "    if VERBOSE:\n",
    "        print(\"Map_attribute_to_id:\"+str(map_attribute_to_id))\n",
    "        print(\"\")\n",
    "        print(\"Map_id_to_attribute:\"+str(map_id_to_attribute))\n",
    "        print(\"\")\n",
    "        print(\"Frequence_per_poi:\"+str(frequence_per_poi_dict))\n",
    "        print(\"\")\n",
    "        print(\"Trajectories: \"+str(trajectory_dict))\n",
    "        print(\"\")\n",
    "        print(\"POI occurring at trajectories: \"+str(poi_at_trajs_dict_set))\n",
    "        print(\"Get data is DONE!\")\n",
    "        \n",
    "    \n",
    "#     D = np.zeros((num_of_objects,num_of_attributes),dtype=int)\n",
    "#     for key, values in poi_at_trajs_dict.items():\n",
    "#         print(\"key:\"+str(key)+\" Values:\"+str(values))\n",
    "#         for line in values:\n",
    "# #             D[line][int(key)] = 1\n",
    "# #             D[line][map_unique_attributes_dataset[key]] = 1\n",
    "# #             print(line,key)\n",
    "# #             print(type(line),type(key))\n",
    "#             D[line][int(key)] = 1\n",
    "#     N = np.zeros((num_of_objects,num_of_attributes),dtype=int)\n",
    "    \n",
    "#     return D, N, poi_at_trajs_dict, data_res_dict, map_id_to_attribute\n",
    "    return map_id_to_attribute, frequence_per_poi_dict, poi_at_trajs_dict_set, trajectory_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fila 1:  <class 'collections.deque'>\n",
      "Fila 2:  deque([{'hotel': 4}, {'padaria': 2}])\n",
      "deque([{'hotel': 4}, {'casa': 7}, {'trabalho': 9}, {'padaria': 2}])\n",
      "deque([{'hotel': 4}, {'casa': 7}, {'trabalho': 9}, {'padaria': 2}, {'festa': 1}])\n",
      "deque([{'aeroporto': 1}, {'hotel': 4}, {'casa': 7}, {'trabalho': 9}, {'padaria': 2}, {'festa': 1}])\n",
      "{'hotel': 4}\n",
      "deque([{'aeroporto': 1}, {'padaria': 3}, {'hotel': 4}, {'casa': 7}, {'trabalho': 9}, {'padaria': 2}, {'festa': 1}])\n",
      "7\n",
      "{'festa': 1}\n",
      "<class 'dict'>\n",
      "{'trabalho': 9}\n",
      "trabalho\n",
      "4\n",
      "Fila 1:  deque([{'aeroporto': 1}, {'padaria': 3}, {'hotel': 4}, {'casa': 7}, {'trabalho': 9}, {'padaria': 2}])\n",
      "Fila 2:  deque([{'hotel': 4}, {'padaria': 2}])\n",
      "Fila 1:  deque([{'padaria': 3}, {'hotel': 4}, {'casa': 7}, {'trabalho': 9}])\n",
      "Fila 1:  deque([{'hotel': 4}, {'casa': 7}, {'trabalho': 9}])\n",
      "Element poped:  {'padaria': 3}\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "my_fila = deque([{'hotel':4},{'casa':7},{'trabalho':9},{'padaria':2}])\n",
    "my_fila2 = deque()\n",
    "my_fila2.append({'hotel':4})\n",
    "my_fila2.append({'padaria':2})\n",
    "print('Fila 1: ',type(my_fila))\n",
    "print('Fila 2: ',my_fila2)\n",
    "print(my_fila)\n",
    "my_fila.append({'festa':1})\n",
    "print(my_fila)\n",
    "my_fila.appendleft({'aeroporto':1})\n",
    "print(my_fila)\n",
    "print(my_fila[1])\n",
    "my_fila.insert(1,{'padaria':3})\n",
    "print(my_fila)\n",
    "print(len(my_fila))\n",
    "print(my_fila.pop())\n",
    "print(type(my_fila[2]))\n",
    "r = my_fila[4]\n",
    "print(r)\n",
    "print(list(r.keys())[0])\n",
    "print(my_fila.index(my_fila[4],2,len(my_fila)))\n",
    "print('Fila 1: ',my_fila)\n",
    "print('Fila 2: ',my_fila2)\n",
    "my_fila.pop()#delete from the right end\n",
    "my_fila.popleft()#delete from the left end\n",
    "print('Fila 1: ',my_fila)\n",
    "f = my_fila.popleft()\n",
    "print('Fila 1: ',my_fila)\n",
    "print('Element poped: ',f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = {'C':4,'B':6,'A':[4,2]}\n",
    "print(g)\n",
    "print(len(g))\n",
    "print('F' in g)\n",
    "print(6 in g)\n",
    "print(g.values())\n",
    "h = set([1,2,2])\n",
    "print(h)\n",
    "h.add(3)\n",
    "print(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too noisy (line,col)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_too_noisy(count_presence, C, e_obj, e_att, att_data_dict, E, dimension):\n",
    "    num_of_atts = len(C[0])\n",
    "    num_of_objs = len(C[1])\n",
    "    if dimension == \"obj\":\n",
    "        # obj must be present in at least (1-e_obj).||C_a||\n",
    "        return count_presence >= ((1-e_obj) * num_of_atts) # return true if the obj is not too noisy\n",
    "    else:\n",
    "        # col must be present in at least (1-e_tt).||C_o||\n",
    "        return count_presence >= ((1-e_att) * num_of_objs) # return true if the att is not too noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(numOfObj, numOfAtt, cov=0,noise=0):\n",
    "    if VERBOSE:\n",
    "        print('Num. objs: {0:2d}, Num. att: {1:2d}, Num. covered: {2:2d}, Num. noise: {2:2d}'.format(numOfObj,numOfAtt,cov,noise))\n",
    "#     return ((numOfObj+numOfAtt) - (numOfObj*numOfAtt)) + cov + (2*noise)\n",
    "    return ((numOfObj+numOfAtt) - (numOfObj*numOfAtt)) + cov + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(SetsC, SetsR, newSetsColumns, newSetsRows):\n",
    "    num_of_cluster = len(SetsC)\n",
    "    \n",
    "    # keep the original cluster\n",
    "    for set_column_i in range(len(SetsC)):\n",
    "        newSetsRows.append(SetsR[set_column_i])\n",
    "        newSetsColumns.append(SetsC[set_column_i])\n",
    "    \n",
    "    revisit = True\n",
    "    while(revisit):\n",
    "        revisit = False\n",
    "        tmp_r = []\n",
    "        tmp_c = []\n",
    "        size = len(newSetsColumns)\n",
    "        for i in range(size-1):\n",
    "            first_r = newSetsRows[i]\n",
    "            first_c = newSetsColumns[i]\n",
    "            for j in range(i+1,size):\n",
    "                p_intersec_r = first_r.intersection(newSetsRows[j])\n",
    "                p_intersec_c = first_c.union(newSetsColumns[j])\n",
    "                if len(p_intersec_r) > 0:\n",
    "                    tmp_r.append(p_intersec_r)\n",
    "                    tmp_c.append(p_intersec_c)\n",
    "        \n",
    "        for i in range(len(tmp_c)):\n",
    "            change = True\n",
    "            for j in range(len(newSetsColumns)):\n",
    "                if tmp_c[i].issubset(newSetsColumns[j]):\n",
    "                    change = False\n",
    "            if change:\n",
    "                newSetsRows.append(tmp_r[i])\n",
    "                newSetsColumns.append(tmp_c[i])\n",
    "                revisit = True\n",
    "    \n",
    "    # start to find and merge all possible overlapping clusters\n",
    "    for set_column_i in range(num_of_cluster):\n",
    "        tested_pattern = SetsR[set_column_i]\n",
    "        new_cols = SetsC[set_column_i]\n",
    "        overlapped = False\n",
    "        \n",
    "        # finds with who the tested pattern overlaps\n",
    "        set_overlap_id = []\n",
    "        for i in range(num_of_cluster):\n",
    "            if i != set_column_i:\n",
    "                if len(tested_pattern.intersection(SetsR[i])) > 0:\n",
    "                    set_overlap_id.append(i)    \n",
    "        \n",
    "        # Discover if exist overlaps between the sets in set_overlap_id\n",
    "        sub_overlap_id = []\n",
    "        for i in range(len(set_overlap_id)):\n",
    "            try_combine_ids = [set_overlap_id[i]]\n",
    "            added = False\n",
    "            for j in range(i+1,len(set_overlap_id)):\n",
    "                if len(SetsR[set_overlap_id[i]].intersection(SetsR[set_overlap_id[j]])) > 0:\n",
    "                    try_combine_ids.append(set_overlap_id[j])\n",
    "                    added = True\n",
    "            sub_overlap_id.append(try_combine_ids)\n",
    "        #Check if some pattern is isolated and was not added\n",
    "        for id_pattern in set_overlap_id:\n",
    "            added = False\n",
    "            for combined_ids in sub_overlap_id:\n",
    "                if id_pattern in combined_ids:\n",
    "                    added = True\n",
    "            if added == False:\n",
    "                sub_overlap_id.append([id_pattern])\n",
    "        \n",
    "        # merge the analysed pattern with the overlapped combined patterns ids\n",
    "        for pattern_ids in sub_overlap_id:\n",
    "            tested_pattern = SetsR[set_column_i]\n",
    "            new_cols = SetsC[set_column_i]\n",
    "            for pattern_id in pattern_ids:\n",
    "                tmp = tested_pattern.intersection(SetsR[pattern_id])\n",
    "                tested_pattern = tmp\n",
    "                new_cols = new_cols.union(SetsC[pattern_id])\n",
    "            # save the new patterns\n",
    "            newSetsColumns.append(new_cols)\n",
    "            newSetsRows.append(tested_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSubsets(setsColumns,setsRows):\n",
    "    finalSetsCols = []\n",
    "    finalSetsRows = []\n",
    "    \n",
    "    again = True\n",
    "    # We are out of the loop when we do not have any subset to remove\n",
    "    while(again):\n",
    "        again = False\n",
    "        for i in range(len(setsColumns)):\n",
    "            isSubset = False\n",
    "            currC = setsColumns[i]\n",
    "            currR = setsRows[i]\n",
    "            \n",
    "            for j in range(len(setsColumns)):\n",
    "                if i != j:\n",
    "                    nextC = setsColumns[j]\n",
    "                    nextR = setsRows[j]\n",
    "                    if currC.issubset(nextC) and currR.issubset(nextR):\n",
    "                        isSubset = True\n",
    "        \n",
    "            if isSubset or len(setsRows[i]) == 0:\n",
    "                setsColumns.pop(i)\n",
    "                setsRows.pop(i)\n",
    "                again = True\n",
    "                break\n",
    "    \n",
    "    #converting data type back to list\n",
    "    for i in range(len(setsColumns)):\n",
    "        setsColumns[i] = list(setsColumns[i])\n",
    "        setsRows[i] = list(setsRows[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort attributes in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted att:  {'0': 100, '45': 45, '10': 10, '65': 9, '87': 2}\n"
     ]
    }
   ],
   "source": [
    "test_dict_freq = {'10':10,'45':45,'65':9,'87':2,'0':100}\n",
    "sorted_attributes = sort_attributes(test_dict_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_attributes(data_res):\n",
    "    \n",
    "    try:\n",
    "        ##usar este for caso o value seja uma lista\n",
    "        freq_res_dict = {}\n",
    "        for key,value in data_res.items():\n",
    "            freq_res_dict[key] = len(value)\n",
    "\n",
    "        # Create a list of tuples sorted by index 1 i.e. value field     \n",
    "        listofTuples = sorted(freq_res_dict.items() , reverse=True, key=lambda x: x[1])# usar se value for lista\n",
    "        # Iterate over the sorted sequence\n",
    "        # for elem in listofTuples :\n",
    "        #     print(elem[0] , \" ::\" , elem[1] )\n",
    "    #     print(listofTuples)\n",
    "        sorted_attributes = [elem[0] for elem in listofTuples]\n",
    "    except:\n",
    "        ## este  usado caso value seja um nmero\n",
    "        sorted_attributes = {k: v for k, v in sorted(data_res.items(), reverse=True, key=lambda item: item[1])}\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(\"Sorted att: \",sorted_attributes)\n",
    "    return sorted_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update residual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_residual_dataset(res_data, attributes_cocluster, objects_cocluster):\n",
    "    for key, value in res_data.items():\n",
    "        if key in attributes_cocluster:\n",
    "            diff_objs = set(res_data[key]).difference(set(objects_cocluster))\n",
    "            res_data[key] = list(diff_objs)\n",
    "    return res_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results - check path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_path(path_method):\n",
    "    current_dir = os.getcwd()\n",
    "    print(current_dir)\n",
    "    res = os.path.exists(path_method)\n",
    "    # clean the folder to save new data\n",
    "    if res:\n",
    "        #check if it is empty\n",
    "        dir_empty = os.listdir(path_method)\n",
    "        if len(dir_empty) != 0:\n",
    "    #         shutil.rmtree(\"OutputAnalysis/kmeans/\")\n",
    "            rm = !rm -r --preserve-root './OutputAnalysis/ococlus/'*\n",
    "            if not rm:\n",
    "                print(\"OCoClus' folder was cleaned.\")\n",
    "    #             os.chdir(path_method)\n",
    "            else:\n",
    "                print(\"sad\")\n",
    "                print(rm)\n",
    "        else:\n",
    "    #         print(\"Empty!\")\n",
    "            pass\n",
    "    #         os.chdir(path_method)\n",
    "    else: # nothing exist so create it\n",
    "        # trying to insert to flase directory \n",
    "        try: \n",
    "    #         os.chdir(fd) \n",
    "            os.mkdir(path_method)\n",
    "            print(\"The path was created: \"+path_method)\n",
    "\n",
    "        # Caching the exception     \n",
    "        except: \n",
    "            print(\"Something wrong with specified directory. Exception- \", sys.exc_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save clustering result into a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def writeFileOutput(cols, rows, dataset, method='OCoClus', fileName='OCoClusResult'):\n",
    "def writeFileOutput(co_clusters, dataset, method='OCoClus', fileName='OCoClusResult'):\n",
    "    text = \"\"\n",
    "#    for c in range(len(data.rows_)):\n",
    "#        res = [i for i, val in enumerate(data.columns_[c]) if val]\n",
    "#        for j in res:\n",
    "#            text += str(j)+\" \"\n",
    "\n",
    "#        res = [i for i, val in enumerate(data.rows_[c]) if val]\n",
    "#        text += \"[\"\n",
    "#        for j in res:\n",
    "#            text += str(j)+\" \"\n",
    "#        text += \"]\\n\"\n",
    "    \n",
    "    num_of_clusters = len(co_clusters)\n",
    "    \n",
    "#     for c in range(len(cols)):\n",
    "    for c in range(num_of_clusters):\n",
    "#         for i in cols[c]:\n",
    "        for i in co_clusters[c][0]: # get the attributes in cluster c\n",
    "            text += str(i)+\" \"\n",
    "        \n",
    "        text += \"(\"+str(len(co_clusters[c][1]))+\") [\" # get the number of objects in clusters c\n",
    "        for j in range(len(co_clusters[c][1])): # save in the file each obj\n",
    "            if j+1 != len(co_clusters[c][1]):\n",
    "                text += str(co_clusters[c][1][j])+\" \"\n",
    "            else:\n",
    "                text += str(co_clusters[c][1][j])\n",
    "        text += \"]\\n\"\n",
    "    \n",
    "    #print(text)\n",
    "    if method == 'Dhillon':\n",
    "        f = open('./datasets/outputs/'+fileName+'.txt', 'w+')#saving at dataset folder\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        print(\"Output file saved in: \"+\"./datasets/outputs/\"+fileName+\".txt\")\n",
    "    elif method == 'Kluger':\n",
    "        f = open('./datasets/outputs/'+fileName+'.txt', 'w+')#saving at dataset folder\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        print(\"Output file saved in: \"+\"./datasets/outputs/\"+fileName+\".txt\")\n",
    "    elif method == 'OCoClus':\n",
    "        f = open('./OutputAnalysis/ococlus/'+dataset+'/'+fileName+'.txt', 'w+')#saving at dataset folder\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        print(\"Output file saved in: \"+\"./OutputAnalysis/ococlus/\"+dataset+\"/\"+fileName+\".txt\")\n",
    "    else:\n",
    "        print(\"The output file was not generated. Method option not recognized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rec_error(data,clusters):\n",
    "    '''\n",
    "    This evaluation measure is computed during the algorithm life time.\n",
    "    '''\n",
    "    reconstructed_ococlus = np.zeros(data.shape,dtype=int)\n",
    "    for nc in range(len(clusters)):\n",
    "        for i in clusters[nc][1]: # object cluster\n",
    "            for j in clusters[nc][0]: # attribute cluster\n",
    "                reconstructed_ococlus[int(i)][int(j)] = 1\n",
    "    print(\"Reconstruction error: \",np.sum(np.bitwise_xor(data,reconstructed_ococlus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omega format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clustering_output_omega(co_clusters):\n",
    "# def build_clustering_output_omega(rowClusters,columnClusters):\n",
    "    '''\n",
    "    Build the clustering output format to use in the omega index evaluation from Remy Cazabet version.\n",
    "    It is optional and we just present this version as a complementary information. If you are interested,\n",
    "    check it out on his team work group at https://github.com/isaranto/omega_index.\n",
    "    '''\n",
    "    \n",
    "    num_of_clusters = len(co_clusters)    \n",
    "    clustering = {}\n",
    "    \n",
    "    for nc in range(num_of_clusters):\n",
    "        rowCluster = co_clusters[nc][1]\n",
    "        columnCluster = co_clusters[nc][0]\n",
    "        clustering[\"c\"+str(nc)] = []\n",
    "        \n",
    "        for i in rowCluster:\n",
    "            for j in columnCluster:\n",
    "                clustering[\"c\"+str(nc)].append((\"01\"+str(i)+\"02\"+str(j)))\n",
    "        \n",
    "    return clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eXascale Infolab \n",
    "We used the xmeasure and OvpNMI project that pushished evaluation measures for overlapping task. We can check it on https://github.com/eXascaleInfolab/xmeasures or https://exascale.info/. Look their project on github to know how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmeasures_format(dict_gt):\n",
    "    '''\n",
    "    This function build the xmeasure format to use it on their evaluation measure.\n",
    "    '''\n",
    "    newData = []\n",
    "    for i in range(len(dict_gt)):\n",
    "#         print(dict_gt['c'+str(i)])\n",
    "        stringLine = dict_gt['c'+str(i)][0]\n",
    "        for j in range(1,len(dict_gt['c'+str(i)])):\n",
    "#             stringLine = stringLine+\" \"+dict_gt['c'+str(i)][j]\n",
    "            stringLine += \" \"+dict_gt['c'+str(i)][j]\n",
    "        newData.append(stringLine)\n",
    "    \n",
    "    return newData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
